{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dzie≈Ñ 2 - Modu≈Ç 4: Nowoczesne modele NLP\n",
    "\n",
    "## Cele modu≈Çu:\n",
    "- Zrozumienie architektury Transformer√≥w\n",
    "- Poznanie modeli BERT, GPT, T5\n",
    "- Fine-tuning gotowych modeli na w≈Çasnych danych\n",
    "- Praktyczne zastosowanie nowoczesnych modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import bibliotek\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel, \n",
    "    AutoModelForSequenceClassification,\n",
    "    pipeline,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"‚úÖ Biblioteki za≈Çadowane!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Architektura Transformer√≥w i ich wp≈Çyw na NLP\n",
    "\n",
    "### Rewolucja w NLP (2017)\n",
    "Paper \"Attention is All You Need\" przedstawi≈Ç architekturƒô Transformer, kt√≥ra:\n",
    "- ZastƒÖpi≈Ça rekurencyjne sieci (RNN/LSTM)\n",
    "- Wprowadzi≈Ç mechanizm uwagi (attention)\n",
    "- Umo≈ºliwi≈Ç przetwarzanie r√≥wnoleg≈Çe\n",
    "- Skaluje siƒô do miliard√≥w parametr√≥w\n",
    "\n",
    "### Kluczowe komponenty:\n",
    "\n",
    "#### 1. Self-Attention (Samo-uwaga)\n",
    "- Ka≈ºde s≈Çowo \"patrzy\" na wszystkie inne s≈Çowa\n",
    "- Oblicza wa≈ºno≈õƒá (attention score) ka≈ºdego s≈Çowa wzglƒôdem innych\n",
    "- Pozwala modelowaƒá d≈Çugodystansowe zale≈ºno≈õci\n",
    "\n",
    "#### 2. Multi-Head Attention\n",
    "- Wiele r√≥wnoleg≈Çych mechanizm√≥w uwagi\n",
    "- Ka≈ºda \"g≈Çowa\" uczy siƒô innych aspekt√≥w relacji\n",
    "\n",
    "#### 3. Positional Encoding\n",
    "- Koduje pozycjƒô s≈Çowa w sekwencji\n",
    "- Brak tego w oryginalnej architekturze (przetwarzanie r√≥wnoleg≈Çe)\n",
    "\n",
    "#### 4. Feed-Forward Networks\n",
    "- Warstwy liniowe z aktywacjƒÖ\n",
    "- Przetwarzanie ka≈ºdej pozycji niezale≈ºnie\n",
    "\n",
    "### Dlaczego Transformery wygra≈Çy?\n",
    "- ‚úÖ **R√≥wnoleg≈Ço≈õƒá** - szybszy trening\n",
    "- ‚úÖ **D≈Çugi kontekst** - lepsze rozumienie\n",
    "- ‚úÖ **Skalowalno≈õƒá** - wiƒôksze modele = lepsza jako≈õƒá\n",
    "- ‚úÖ **Transfer learning** - pre-training + fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Wizualizacja: Jak dzia≈Ça attention?\n",
    "# Prosty przyk≈Çad ilustrujƒÖcy mechanizm uwagi\n",
    "\n",
    "def simple_attention_visualization():\n",
    "    \"\"\"\n",
    "    Prosta wizualizacja mechanizmu uwagi.\n",
    "    \"\"\"\n",
    "    # Przyk≈Çadowe zdanie\n",
    "    words = [\"Kot\", \"siedzi\", \"na\", \"macie\"]\n",
    "    \n",
    "    # Symulowana macierz uwagi (attention matrix)\n",
    "    # Ka≈ºdy wiersz pokazuje jak dane s≈Çowo \"zwraca uwagƒô\" na inne s≈Çowa\n",
    "    attention_matrix = np.array([\n",
    "        [0.7, 0.1, 0.1, 0.1],  # \"Kot\" zwraca uwagƒô g≈Ç√≥wnie na siebie\n",
    "        [0.3, 0.5, 0.1, 0.1],  # \"siedzi\" zwraca uwagƒô na \"Kot\" i siebie\n",
    "        [0.1, 0.2, 0.4, 0.3],  # \"na\" ≈ÇƒÖczy \"siedzi\" z \"macie\"\n",
    "        [0.2, 0.1, 0.2, 0.5],  # \"macie\" zwraca uwagƒô g≈Ç√≥wnie na siebie\n",
    "    ])\n",
    "    \n",
    "    # Wizualizacja\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(attention_matrix, \n",
    "                xticklabels=words, \n",
    "                yticklabels=words,\n",
    "                annot=True, \n",
    "                fmt='.2f',\n",
    "                cmap='YlOrRd',\n",
    "                cbar_kws={'label': 'Attention Score'})\n",
    "    plt.title('Attention Matrix - \"Kot siedzi na macie\"', fontsize=14)\n",
    "    plt.xlabel('S≈Çowa (Key)')\n",
    "    plt.ylabel('S≈Çowa (Query)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Interpretacja:\")\n",
    "    print(\"- Ja≈õniejsze kolory = silniejsza uwaga\")\n",
    "    print(\"- 'siedzi' zwraca du≈ºƒÖ uwagƒô na 'Kot' (podmiot czasownika)\")\n",
    "    print(\"- 'na' ≈ÇƒÖczy czasownik z dope≈Çnieniem\")\n",
    "\n",
    "simple_attention_visualization()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Model BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "### Charakterystyka BERT:\n",
    "- **Bidirectional** - czyta tekst w obu kierunkach jednocze≈õnie\n",
    "- **Encoder-only** - tylko czƒô≈õƒá enkoder z Transformera\n",
    "- **Pre-training** - uczony na ogromnych korpusach tekstu\n",
    "- **Fine-tuning** - ≈Çatwo adaptowaƒá do konkretnych zada≈Ñ\n",
    "\n",
    "### Pre-training BERT:\n",
    "1. **Masked Language Modeling (MLM)**\n",
    "   - 15% s≈Ç√≥w jest maskowanych [MASK]\n",
    "   - Model pr√≥buje odgadnƒÖƒá zamaskowane s≈Çowa\n",
    "   - Przyk≈Çad: \"Kot siedzi na [MASK]\" ‚Üí \"macie\"\n",
    "\n",
    "2. **Next Sentence Prediction (NSP)**\n",
    "   - Czy zdanie B nastƒôpuje po zdaniu A?\n",
    "   - Pomaga w zadaniach wymagajƒÖcych rozumienia relacji miƒôdzy zdaniami\n",
    "\n",
    "### Warianty BERT:\n",
    "- **BERT-base**: 110M parametr√≥w\n",
    "- **BERT-large**: 340M parametr√≥w\n",
    "- **RoBERTa**: Ulepszona wersja BERT\n",
    "- **DistilBERT**: Mniejsza, szybsza wersja\n",
    "- **HerBERT**: Specjalnie dla jƒôzyka polskiego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad 1: BERT - Fill-Mask (uzupe≈Çnianie brakujƒÖcych s≈Ç√≥w)\n",
    "print(\"=== BERT: FILL-MASK ===\")\n",
    "\n",
    "# Model angielski\n",
    "fill_mask = pipeline(\"fill-mask\", model=\"bert-base-uncased\")\n",
    "\n",
    "# Przyk≈Çady\n",
    "examples = [\n",
    "    \"Paris is the [MASK] of France.\",\n",
    "    \"The [MASK] is shining brightly in the sky.\",\n",
    "    \"I love to [MASK] music.\",\n",
    "]\n",
    "\n",
    "for example in examples:\n",
    "    print(f\"\\nZdanie: {example}\")\n",
    "    results = fill_mask(example, top_k=3)\n",
    "    print(\"Top 3 predykcje:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"  {i}. {result['token_str']:15} (score: {result['score']:.4f})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad 2: BERT Embeddings - reprezentacja wektorowa tekstu\n",
    "print(\"\\n=== BERT: EMBEDDINGS ===\")\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Tekst do zakodowania\n",
    "texts = [\n",
    "    \"I love natural language processing.\",\n",
    "    \"I enjoy working with NLP.\",\n",
    "    \"The weather is nice today.\"\n",
    "]\n",
    "\n",
    "# Funkcja do uzyskania embedding√≥w\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenizacja\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # U≈ºyj [CLS] token embedding jako reprezentacji ca≈Çego zdania\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding\n",
    "\n",
    "# Uzyskaj embeddingi\n",
    "embeddings = [get_bert_embeddings(text) for text in texts]\n",
    "\n",
    "print(f\"\\nShape embeddingu: {embeddings[0].shape}\")\n",
    "print(f\"Ka≈ºdy tekst reprezentowany przez wektor {embeddings[0].shape[1]} liczb\")\n",
    "\n",
    "# Oblicz podobie≈Ñstwo cosinusowe\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "print(\"\\n=== PODOBIE≈ÉSTWA ===\")\n",
    "sim_1_2 = cosine_similarity(embeddings[0], embeddings[1]).item()\n",
    "sim_1_3 = cosine_similarity(embeddings[0], embeddings[2]).item()\n",
    "sim_2_3 = cosine_similarity(embeddings[1], embeddings[2]).item()\n",
    "\n",
    "print(f\"Tekst 1 <-> Tekst 2: {sim_1_2:.4f}\")\n",
    "print(f\"Tekst 1 <-> Tekst 3: {sim_1_3:.4f}\")\n",
    "print(f\"Tekst 2 <-> Tekst 3: {sim_2_3:.4f}\")\n",
    "print(\"\\nüí° Teksty 1 i 2 sƒÖ najbardziej podobne (oba o NLP)!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Model GPT (Generative Pre-trained Transformer)\n",
    "\n",
    "### Charakterystyka GPT:\n",
    "- **Autoregressive** - generuje tekst s≈Çowo po s≈Çowie\n",
    "- **Decoder-only** - tylko czƒô≈õƒá dekoder z Transformera\n",
    "- **Causal attention** - ka≈ºde s≈Çowo widzi tylko poprzednie s≈Çowa\n",
    "- **Few-shot learning** - mo≈ºe uczyƒá siƒô z kilku przyk≈Çad√≥w\n",
    "\n",
    "### Ewolucja GPT:\n",
    "- **GPT-1** (2018): 117M parametr√≥w\n",
    "- **GPT-2** (2019): 1.5B parametr√≥w\n",
    "- **GPT-3** (2020): 175B parametr√≥w\n",
    "- **GPT-3.5** (2022): Podstawa ChatGPT\n",
    "- **GPT-4** (2023): Multimodal, jeszcze potƒô≈ºniejszy\n",
    "\n",
    "### Pre-training GPT:\n",
    "- **Causal Language Modeling**\n",
    "- Przewidywanie nastƒôpnego s≈Çowa\n",
    "- Przyk≈Çad: \"Kot siedzi na\" ‚Üí \"macie\"\n",
    "\n",
    "### BERT vs GPT:\n",
    "\n",
    "| Aspekt | BERT | GPT |\n",
    "|--------|------|-----|\n",
    "| Architektura | Encoder | Decoder |\n",
    "| Kierunek | Bidirectional | Unidirectional (left-to-right) |\n",
    "| Pre-training | MLM, NSP | Causal LM |\n",
    "| G≈Ç√≥wne zastosowanie | Rozumienie | Generowanie |\n",
    "| Kontekst | Ca≈Çe zdanie | Poprzednie s≈Çowa |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad: GPT-2 - generowanie tekstu\n",
    "print(\"=== GPT-2: GENEROWANIE TEKSTU ===\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompts = [\n",
    "    \"Artificial intelligence will\",\n",
    "    \"In the future, technology\",\n",
    "    \"Natural language processing is\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    results = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=2,\n",
    "        temperature=0.8,  # Kontrola \"kreatywno≈õci\" (0 = deterministyczne, 1+ = kreatywne)\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nWersja {i}:\")\n",
    "        print(result['generated_text'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Model T5 (Text-to-Text Transfer Transformer)\n",
    "\n",
    "### Filozofia T5:\n",
    "- **Wszystko jako Text-to-Text**\n",
    "- Ka≈ºde zadanie NLP to przekszta≈Çcenie tekstu na tekst\n",
    "- Uniwersalny format treningowy\n",
    "\n",
    "### Przyk≈Çady zada≈Ñ:\n",
    "- **T≈Çumaczenie**: \"translate English to German: Hello\" ‚Üí \"Hallo\"\n",
    "- **Podsumowanie**: \"summarize: [d≈Çugi tekst]\" ‚Üí [kr√≥tki tekst]\n",
    "- **Klasyfikacja**: \"sentiment: This is great!\" ‚Üí \"positive\"\n",
    "\n",
    "### Architektura:\n",
    "- **Encoder-Decoder** (pe≈Çny Transformer)\n",
    "- Po≈ÇƒÖczenie zalet BERT i GPT\n",
    "- Encoder rozumie, Decoder generuje\n",
    "\n",
    "### Warianty:\n",
    "- **T5-small**: 60M parametr√≥w\n",
    "- **T5-base**: 220M parametr√≥w\n",
    "- **T5-large**: 770M parametr√≥w\n",
    "- **T5-3B/11B**: Najwiƒôksze wersje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad: T5 - r√≥≈ºne zadania\n",
    "print(\"=== T5: TEXT-TO-TEXT ===\")\n",
    "\n",
    "# Podsumowanie\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "long_text = \"\"\"\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, \n",
    "and artificial intelligence concerned with the interactions between computers and \n",
    "human language. It involves programming computers to process and analyze large \n",
    "amounts of natural language data. Challenges in NLP frequently involve speech \n",
    "recognition, natural language understanding, and natural language generation. \n",
    "Modern NLP techniques are based on machine learning, especially deep learning.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- PODSUMOWANIE ---\")\n",
    "summary = summarizer(long_text, max_length=50, min_length=20)\n",
    "print(f\"Orygina≈Ç ({len(long_text.split())} s≈Ç√≥w):\")\n",
    "print(long_text.strip())\n",
    "print(f\"\\nPodsumowanie:\")\n",
    "print(summary[0]['summary_text'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# T5 - T≈Çumaczenie (je≈õli dostƒôpny model)\n",
    "try:\n",
    "    translator = pipeline(\"translation_en_to_de\", model=\"t5-small\")\n",
    "    \n",
    "    print(\"\\n--- T≈ÅUMACZENIE (EN -> DE) ---\")\n",
    "    english_text = \"Hello, how are you today?\"\n",
    "    translation = translator(english_text)\n",
    "    print(f\"English: {english_text}\")\n",
    "    print(f\"German: {translation[0]['translation_text']}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è T≈Çumaczenie niedostƒôpne: {e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Fine-tuning gotowych modeli na w≈Çasnych danych\n",
    "\n",
    "### Co to jest Fine-tuning?\n",
    "- Dostosowanie pre-trenowanego modelu do konkretnego zadania\n",
    "- Du≈ºo szybsze i ta≈Ñsze ni≈º trening od zera\n",
    "- Wymaga mniej danych ni≈º trening od podstaw\n",
    "\n",
    "### Kroki fine-tuningu:\n",
    "1. Wyb√≥r pre-trenowanego modelu\n",
    "2. Przygotowanie danych treningowych\n",
    "3. Konfiguracja parametr√≥w treningu\n",
    "4. Trening (fine-tuning)\n",
    "5. Ewaluacja\n",
    "6. Zapisanie i wdro≈ºenie modelu\n",
    "\n",
    "### Kiedy fine-tunowaƒá?\n",
    "- ‚úÖ Masz specyficznƒÖ domenƒô (medycyna, prawo, finanse)\n",
    "- ‚úÖ Masz etykietowane dane treningowe\n",
    "- ‚úÖ Og√≥lne modele nie dajƒÖ wystarczajƒÖcej jako≈õci\n",
    "- ‚úÖ Potrzebujesz specyficznego formatu wyj≈õcia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad: Fine-tuning BERT do klasyfikacji sentymentu\n",
    "print(\"=== FINE-TUNING: KLASYFIKACJA SENTYMENTU ===\")\n",
    "\n",
    "# 1. Przygotowanie danych\n",
    "train_data = [\n",
    "    {\"text\": \"This product is amazing! I love it!\", \"label\": 1},\n",
    "    {\"text\": \"Terrible experience. Never buying again.\", \"label\": 0},\n",
    "    {\"text\": \"Great quality and fast shipping.\", \"label\": 1},\n",
    "    {\"text\": \"Worst purchase ever. Completely broken.\", \"label\": 0},\n",
    "    {\"text\": \"Absolutely fantastic! Highly recommend!\", \"label\": 1},\n",
    "    {\"text\": \"Poor quality. Very disappointed.\", \"label\": 0},\n",
    "    {\"text\": \"Excellent product. Worth every penny.\", \"label\": 1},\n",
    "    {\"text\": \"Waste of money. Don't buy this.\", \"label\": 0},\n",
    "]\n",
    "\n",
    "# Konwersja do Dataset\n",
    "dataset = Dataset.from_list(train_data)\n",
    "\n",
    "print(\"Dataset:\")\n",
    "print(dataset)\n",
    "print(f\"\\nPrzyk≈Çad: {dataset[0]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 2. Przygotowanie modelu i tokenizera\n",
    "model_name = \"distilbert-base-uncased\"  # Mniejszy, szybszy BERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2  # binary classification\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Model za≈Çadowany: {model_name}\")\n",
    "print(f\"Parametry: {sum(p.numel() for p in model.parameters()):,}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 3. Tokenizacja danych\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"], \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(\"\\n‚úÖ Dane ztokenizowane\")\n",
    "print(f\"Kolumny: {tokenized_dataset.column_names}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 4. Konfiguracja treningu\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    warmup_steps=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",  # Nie zapisuj checkpoint√≥w (dla demonstracji)\n",
    ")\n",
    "\n",
    "# 5. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Trainer skonfigurowany\")\n",
    "print(\"\\n‚ö†Ô∏è W prawdziwym projekcie teraz uruchomiliby≈õmy: trainer.train()\")\n",
    "print(\"(Pomijamy trening w demonstracji ze wzglƒôdu na czas)\")\n",
    "\n",
    "# W prawdziwym scenariuszu:\n",
    "# trainer.train()\n",
    "# model.save_pretrained(\"./my_finetuned_model\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# 6. Testowanie (z pre-trenowanym modelem)\n",
    "# U≈ºyjmy gotowego fine-tunowanego modelu do sentymentu\n",
    "print(\"\\n=== TESTOWANIE FINE-TUNOWANEGO MODELU ===\")\n",
    "\n",
    "sentiment_pipeline = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "test_texts = [\n",
    "    \"This is absolutely wonderful!\",\n",
    "    \"I hate this product.\",\n",
    "    \"It's okay, nothing special.\",\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = sentiment_pipeline(text)[0]\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"  ‚Üí {result['label']}: {result['score']:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ƒÜwiczenie praktyczne\n",
    "\n",
    "### Zadanie:\n",
    "1. Wybierz pre-trenowany model (BERT, GPT, T5)\n",
    "2. U≈ºyj go do konkretnego zadania:\n",
    "   - BERT: Klasyfikacja lub ekstrakcja informacji\n",
    "   - GPT: Generowanie tekstu\n",
    "   - T5: Podsumowanie lub t≈Çumaczenie\n",
    "3. Eksperymentuj z r√≥≈ºnymi parametrami\n",
    "4. Por√≥wnaj wyniki r√≥≈ºnych modeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# MIEJSCE NA TW√ìJ KOD\n",
    "# Eksperymentuj z r√≥≈ºnymi modelami!\n",
    "\n",
    "print(\"üí° Wskaz√≥wki:\")\n",
    "print(\"- U≈ºyj Hugging Face Model Hub: https://huggingface.co/models\")\n",
    "print(\"- Szukaj modeli dla swojego jƒôzyka (np. 'polish bert')\")\n",
    "print(\"- Sprawd≈∫ dokumentacjƒô modelu przed u≈ºyciem\")\n",
    "print(\"- Eksperymentuj z parametrami (temperature, max_length, etc.)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie Modu≈Çu 4\n",
    "\n",
    "‚úÖ Zrozumieli≈õmy architekturƒô Transformer√≥w i mechanizm attention\n",
    "\n",
    "‚úÖ Poznali≈õmy g≈Ç√≥wne modele: BERT (rozumienie), GPT (generowanie), T5 (text-to-text)\n",
    "\n",
    "‚úÖ Nauczyli≈õmy siƒô u≈ºywaƒá pre-trenowanych modeli\n",
    "\n",
    "‚úÖ Poznali≈õmy proces fine-tuningu w≈Çasnych modeli\n",
    "\n",
    "‚úÖ Wykonali≈õmy praktyczne przyk≈Çady z ka≈ºdym typem modelu\n",
    "\n",
    "### Kluczowe r√≥≈ºnice:\n",
    "\n",
    "| Model | Architektura | Zastosowanie | Kierunek |\n",
    "|-------|-------------|--------------|----------|\n",
    "| BERT | Encoder | Rozumienie, klasyfikacja | Bidirectional |\n",
    "| GPT | Decoder | Generowanie | Unidirectional |\n",
    "| T5 | Encoder-Decoder | Uniwersalne | Both |\n",
    "\n",
    "---\n",
    "\n",
    "**Nastƒôpny krok**: Modu≈Ç 5 - Generowanie i rozumienie tekstu w praktyce"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
