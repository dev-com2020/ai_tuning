{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dzie≈Ñ 1 - Warsztaty Praktyczne\n",
    "\n",
    "## Cele warsztat√≥w:\n",
    "- Zastosowanie poznanych technik w praktycznych projektach\n",
    "- Tokenizacja i analiza danych tekstowych\n",
    "- Eksploracja i przygotowanie danych\n",
    "- Budowa prostego modelu klasyfikacji tekstu\n",
    "\n",
    "## Projekty do realizacji:\n",
    "1. Analiza sentymentu recenzji\n",
    "2. Klasyfikacja tekst√≥w (spam detection)\n",
    "3. Eksploracja korpusu tekstowego\n",
    "4. Mini-projekt: Budowa klasyfikatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import wszystkich potrzebnych bibliotek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Konfiguracja\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Pobierz zasoby NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"‚úÖ Biblioteki za≈Çadowane!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekt 1: Analiza Sentymentu Recenzji\n",
    "\n",
    "### Cel:\n",
    "Przeanalizuj recenzje produkt√≥w i okre≈õl ich sentyment (pozytywny/negatywny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stw√≥rzmy zbi√≥r przyk≈Çadowych recenzji\n",
    "recenzje = [\n",
    "    (\"≈öwietny produkt! Bardzo jestem zadowolony. Polecam!\", \"pozytywny\"),\n",
    "    (\"Okropne. Nie dzia≈Ça tak jak powinno. Zwracam.\", \"negatywny\"),\n",
    "    (\"Ca≈Çkiem niez≈Çy, ale cena mog≈Çaby byƒá ni≈ºsza.\", \"neutralny\"),\n",
    "    (\"Najlepszy zakup w tym roku! Fantastyczna jako≈õƒá!\", \"pozytywny\"),\n",
    "    (\"Totalna pora≈ºka. Strata pieniƒôdzy.\", \"negatywny\"),\n",
    "    (\"Produkt jest OK, nic specjalnego.\", \"neutralny\"),\n",
    "    (\"Rewelacja! Dok≈Çadnie to czego szuka≈Çem!\", \"pozytywny\"),\n",
    "    (\"Nie polecam. S≈Çaba jako≈õƒá wykonania.\", \"negatywny\"),\n",
    "    (\"Za tƒô cenƒô spodziewa≈Çem siƒô wiƒôcej.\", \"negatywny\"),\n",
    "    (\"Solidny produkt. Spe≈Çnia oczekiwania.\", \"pozytywny\"),\n",
    "]\n",
    "\n",
    "# Utw√≥rz DataFrame\n",
    "df = pd.DataFrame(recenzje, columns=['tekst', 'sentyment'])\n",
    "\n",
    "print(\"=== ZBI√ìR DANYCH ===\")\n",
    "print(df)\n",
    "print(f\"\\nRozk≈Çad klas:\")\n",
    "print(df['sentyment'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analiza eksploracyjna\n",
    "print(\"\\n=== STATYSTYKI PODSTAWOWE ===\")\n",
    "\n",
    "# D≈Çugo≈õƒá tekst√≥w\n",
    "df['dlugosc'] = df['tekst'].apply(len)\n",
    "df['liczba_slow'] = df['tekst'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(df[['tekst', 'sentyment', 'dlugosc', 'liczba_slow']].head())\n",
    "\n",
    "# Statystyki\n",
    "print(\"\\n≈örednia d≈Çugo≈õƒá tekstu:\", df['dlugosc'].mean())\n",
    "print(\"≈örednia liczba s≈Ç√≥w:\", df['liczba_slow'].mean())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Wizualizacja rozk≈Çadu sentymentu\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['sentyment'].value_counts().plot(kind='bar')\n",
    "plt.title('Rozk≈Çad sentymentu')\n",
    "plt.xlabel('Sentyment')\n",
    "plt.ylabel('Liczba recenzji')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df.groupby('sentyment')['liczba_slow'].mean().plot(kind='bar', color='coral')\n",
    "plt.title('≈örednia liczba s≈Ç√≥w wed≈Çug sentymentu')\n",
    "plt.xlabel('Sentyment')\n",
    "plt.ylabel('≈örednia liczba s≈Ç√≥w')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analiza najczƒôstszych s≈Ç√≥w\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenizacja i liczenie s≈Ç√≥w\n",
    "wszystkie_slowa = []\n",
    "stop_words = set(stopwords.words('polish'))\n",
    "\n",
    "for tekst in df['tekst']:\n",
    "    tokens = word_tokenize(tekst.lower(), language='polish')\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    wszystkie_slowa.extend(tokens)\n",
    "\n",
    "# Najczƒôstsze s≈Çowa\n",
    "najczestsze = Counter(wszystkie_slowa).most_common(15)\n",
    "\n",
    "print(\"\\n=== 15 NAJCZƒòSTSZYCH S≈Å√ìW ===\")\n",
    "for slowo, liczba in najczestsze:\n",
    "    print(f\"{slowo:20} : {liczba}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# WordCloud - chmura s≈Ç√≥w\n",
    "try:\n",
    "    tekst_caly = ' '.join(wszystkie_slowa)\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          colormap='viridis').generate(tekst_caly)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Chmura s≈Ç√≥w z recenzji', fontsize=16)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"B≈ÇƒÖd generowania chmury s≈Ç√≥w: {e}\")\n",
    "    print(\"Zainstaluj: pip install wordcloud\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekt 2: Klasyfikacja Spam vs Ham (SMS)\n",
    "\n",
    "### Cel:\n",
    "Zbuduj prosty klasyfikator do rozr√≥≈ºniania spamu od prawid≈Çowych wiadomo≈õci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çadowy dataset SMS\n",
    "sms_data = [\n",
    "    (\"Wygra≈Çe≈õ 1000 z≈Ç! Kliknij link aby odebraƒá nagrodƒô!\", \"spam\"),\n",
    "    (\"Cze≈õƒá, spotkamy siƒô jutro o 18?\", \"ham\"),\n",
    "    (\"GRATULACJE! Zosta≈Çe≈õ wybrany do otrzymania iPhone'a!\", \"spam\"),\n",
    "    (\"Pamiƒôtaj o spotkaniu z klientem w czwartek\", \"ham\"),\n",
    "    (\"Kup teraz! Wielka promocja tylko dzi≈õ! -90%\", \"spam\"),\n",
    "    (\"Dziƒôki za wczorajsze spotkanie. Pozdrawiam\", \"ham\"),\n",
    "    (\"Pilne! Twoje konto zosta≈Ço zablokowane. Kliknij tutaj\", \"spam\"),\n",
    "    (\"Mama dzwoni≈Ça, oddzwo≈Ñ jak bƒôdziesz mia≈Ç czas\", \"ham\"),\n",
    "    (\"DARMOWE PIENIƒÑDZE! Nie czekaj, zarejestruj siƒô!\", \"spam\"),\n",
    "    (\"Kupi≈Çem mleko, wracam za godzinƒô\", \"ham\"),\n",
    "    (\"Ostatnia szansa! Oferta wygasa za 2 godziny!!!\", \"spam\"),\n",
    "    (\"Spotkanie przeniesione na piƒÖtek o 14:00\", \"ham\"),\n",
    "]\n",
    "\n",
    "df_sms = pd.DataFrame(sms_data, columns=['wiadomosc', 'kategoria'])\n",
    "\n",
    "print(\"=== DATASET SMS ===\")\n",
    "print(df_sms)\n",
    "print(f\"\\nRozk≈Çad klas:\")\n",
    "print(df_sms['kategoria'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocessing - pipeline\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Przetwarzanie tekstu przed klasyfikacjƒÖ.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenizacja\n",
    "    tokens = word_tokenize(text, language='polish')\n",
    "    \n",
    "    # Usu≈Ñ znaki specjalne i liczby\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    \n",
    "    # Usu≈Ñ stop words\n",
    "    stop_words = set(stopwords.words('polish'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Zastosuj preprocessing\n",
    "df_sms['wiadomosc_przetworzona'] = df_sms['wiadomosc'].apply(preprocess_text)\n",
    "\n",
    "print(\"=== PRZYK≈ÅADY PRZETWARZANIA ===\")\n",
    "for idx in [0, 1, 4]:\n",
    "    print(f\"\\nOrygina≈Ç: {df_sms.iloc[idx]['wiadomosc']}\")\n",
    "    print(f\"Przetworzone: {df_sms.iloc[idx]['wiadomosc_przetworzona']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature Engineering - TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=50)\n",
    "\n",
    "X = vectorizer.fit_transform(df_sms['wiadomosc_przetworzona'])\n",
    "y = df_sms['kategoria']\n",
    "\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "print(f\"Shape macierzy cech: {X.shape}\")\n",
    "print(f\"\\nNajwa≈ºniejsze cechy (s≈Çowa):\")\n",
    "print(vectorizer.get_feature_names_out()[:20])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Podzia≈Ç na zbi√≥r treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"=== PODZIA≈Å DANYCH ===\")\n",
    "print(f\"Zbi√≥r treningowy: {X_train.shape[0]} pr√≥bek\")\n",
    "print(f\"Zbi√≥r testowy: {X_test.shape[0]} pr√≥bek\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Trening modelu - Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predykcja\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== WYNIKI MODELU ===\")\n",
    "print(f\"\\nDok≈Çadno≈õƒá: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "print(\"\\nRaport klasyfikacji:\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test modelu na nowych danych\n",
    "nowe_wiadomosci = [\n",
    "    \"Wygra≈Çe≈õ milion z≈Çotych! Kliknij teraz!\",\n",
    "    \"Spotkamy siƒô w kawiarni o 16?\",\n",
    "    \"PROMOCJA! Kup teraz ze zni≈ºkƒÖ 99%\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== TEST NA NOWYCH DANYCH ===\")\n",
    "for wiadomosc in nowe_wiadomosci:\n",
    "    przetworzona = preprocess_text(wiadomosc)\n",
    "    wektor = vectorizer.transform([przetworzona])\n",
    "    predykcja = model.predict(wektor)[0]\n",
    "    prawdopodobienstwo = model.predict_proba(wektor)[0]\n",
    "    \n",
    "    print(f\"\\nWiadomo≈õƒá: {wiadomosc}\")\n",
    "    print(f\"Predykcja: {predykcja}\")\n",
    "    print(f\"Prawdopodobie≈Ñstwo: ham={prawdopodobienstwo[0]:.2%}, spam={prawdopodobienstwo[1]:.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekt 3: Eksploracja korpusu tekstowego\n",
    "\n",
    "### Cel:\n",
    "Przeanalizuj wiƒôkszy zbi√≥r tekst√≥w i wyciƒÖgnij interesujƒÖce wnioski."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Utw√≥rz wiƒôkszy korpus (symulacja artyku≈Ç√≥w)\n",
    "artykuly = [\n",
    "    {\n",
    "        'tytul': 'Sztuczna inteligencja w medycynie',\n",
    "        'tekst': 'Sztuczna inteligencja rewolucjonizuje medycynƒô. Algorytmy uczenia maszynowego pomagajƒÖ w diagnostyce chor√≥b. Systemy AI analizujƒÖ obrazy medyczne z dok≈Çadno≈õciƒÖ przewy≈ºszajƒÖcƒÖ ludzkich ekspert√≥w.',\n",
    "        'kategoria': 'technologia'\n",
    "    },\n",
    "    {\n",
    "        'tytul': 'Nowe odkrycie archeologiczne',\n",
    "        'tekst': 'Archeolodzy odkryli staro≈ºytne miasto. Znaleziska obejmujƒÖ ceramikƒô, narzƒôdzia i ruiny budynk√≥w. Odkrycie rzuca nowe ≈õwiat≈Ço na historiƒô staro≈ºytnych cywilizacji.',\n",
    "        'kategoria': 'historia'\n",
    "    },\n",
    "    {\n",
    "        'tytul': 'Przysz≈Ço≈õƒá transportu',\n",
    "        'tekst': 'Pojazdy autonomiczne to przysz≈Ço≈õƒá transportu. Samochody bez kierowcy bƒôdƒÖ bezpieczniejsze i bardziej efektywne. Technologia ta zmieni spos√≥b w jaki siƒô przemieszczamy.',\n",
    "        'kategoria': 'technologia'\n",
    "    },\n",
    "    {\n",
    "        'tytul': 'Zmiany klimatyczne',\n",
    "        'tekst': 'Globalne ocieplenie wp≈Çywa na ekosystemy. Naukowcy ostrzegajƒÖ przed konsekwencjami zmian klimatycznych. Konieczne sƒÖ pilne dzia≈Çania aby chroniƒá naszƒÖ planetƒô.',\n",
    "        'kategoria': '≈õrodowisko'\n",
    "    },\n",
    "]\n",
    "\n",
    "df_artykuly = pd.DataFrame(artykuly)\n",
    "print(\"=== KORPUS ARTYKU≈Å√ìW ===\")\n",
    "print(df_artykuly[['tytul', 'kategoria']])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analiza z u≈ºyciem spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"pl_core_news_sm\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Zainstaluj model polski: python -m spacy download pl_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "if nlp:\n",
    "    print(\"\\n=== ANALIZA LINGUISTYCZNA ===\")\n",
    "    \n",
    "    for idx, row in df_artykuly.iterrows():\n",
    "        doc = nlp(row['tekst'])\n",
    "        \n",
    "        print(f\"\\n{row['tytul']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Rzeczowniki\n",
    "        rzeczowniki = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "        print(f\"Kluczowe rzeczowniki: {list(set(rzeczowniki))[:5]}\")\n",
    "        \n",
    "        # Nazwane encje\n",
    "        if doc.ents:\n",
    "            encje = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "            print(f\"Encje: {encje}\")\n",
    "        \n",
    "        # Statystyki\n",
    "        print(f\"Liczba zda≈Ñ: {len(list(doc.sents))}\")\n",
    "        print(f\"Liczba token√≥w: {len(doc)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ƒÜwiczenie finalne: Mini-projekt\n",
    "\n",
    "### Zadanie:\n",
    "Stw√≥rz kompletny pipeline klasyfikacji tekstu:\n",
    "1. Za≈Çaduj lub stw√≥rz dane\n",
    "2. Wykonaj eksploracyjnƒÖ analizƒô danych (EDA)\n",
    "3. Przetw√≥rz tekst (preprocessing)\n",
    "4. Wyekstraktuj cechy (TF-IDF lub inne)\n",
    "5. Wytrenuj model (wybierz algorytm)\n",
    "6. Oce≈Ñ model (metryki, confusion matrix)\n",
    "7. Przetestuj na nowych danych\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºywaj poznanych technik z ca≈Çego dnia\n",
    "- Eksperymentuj z r√≥≈ºnymi parametrami\n",
    "- Por√≥wnaj r√≥≈ºne modele (Naive Bayes, Logistic Regression)\n",
    "- Wizualizuj wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TWOJA IMPLEMENTACJA\n",
    "# Rozpocznij tutaj swojƒÖ pracƒô nad mini-projektem\n",
    "\n",
    "# Przyk≈Çadowa struktura:\n",
    "\n",
    "# 1. Za≈Çaduj dane\n",
    "# data = ...\n",
    "\n",
    "# 2. EDA\n",
    "# ...\n",
    "\n",
    "# 3. Preprocessing\n",
    "# ...\n",
    "\n",
    "# 4. Feature extraction\n",
    "# ...\n",
    "\n",
    "# 5. Model training\n",
    "# ...\n",
    "\n",
    "# 6. Evaluation\n",
    "# ...\n",
    "\n",
    "# 7. Testing\n",
    "# ...\n",
    "\n",
    "print(\"üöÄ Powodzenia w realizacji projektu!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodatkowe wyzwania (opcjonalne)\n",
    "\n",
    "### Wyzwanie 1: Por√≥wnanie modeli\n",
    "Por√≥wnaj wydajno≈õƒá r√≥≈ºnych algorytm√≥w:\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Random Forest\n",
    "\n",
    "### Wyzwanie 2: Feature engineering\n",
    "Spr√≥buj r√≥≈ºnych metod ekstrakcji cech:\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "- N-gramy (bigrams, trigrams)\n",
    "- Word embeddings\n",
    "\n",
    "### Wyzwanie 3: Optymalizacja\n",
    "U≈ºyj GridSearchCV do znalezienia najlepszych hiperparametr√≥w.\n",
    "\n",
    "### Wyzwanie 4: Analiza b≈Çƒôd√≥w\n",
    "Przeanalizuj przypadki w kt√≥rych model siƒô myli - co mo≈ºna poprawiƒá?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie Dnia 1\n",
    "\n",
    "### Czego siƒô nauczyli≈õmy:\n",
    "‚úÖ Podstawy NLP i jego zastosowa≈Ñ\n",
    "\n",
    "‚úÖ Narzƒôdzia: NLTK, spaCy, Hugging Face, OpenAI\n",
    "\n",
    "‚úÖ Operacje: tokenizacja, lematyzacja, POS tagging\n",
    "\n",
    "‚úÖ Praktyczne projekty: analiza sentymentu, spam detection\n",
    "\n",
    "‚úÖ Pipeline ML: od danych do modelu\n",
    "\n",
    "### Na jutro:\n",
    "üöÄ Nowoczesne modele NLP (Transformery)\n",
    "\n",
    "üöÄ BERT, GPT i fine-tuning\n",
    "\n",
    "üöÄ Generowanie i rozumienie tekstu\n",
    "\n",
    "üöÄ Zastosowania biznesowe (chatboty, automatyzacja)\n",
    "\n",
    "---\n",
    "\n",
    "**Dobra robota! üéâ Do zobaczenia jutro!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
