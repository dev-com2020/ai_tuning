{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dzień 1 - Warsztaty Praktyczne\n",
    "\n",
    "## Cele warsztatów:\n",
    "- Zastosowanie poznanych technik w praktycznych projektach\n",
    "- Tokenizacja i analiza danych tekstowych\n",
    "- Eksploracja i przygotowanie danych\n",
    "- Budowa prostego modelu klasyfikacji tekstu\n",
    "\n",
    "## Projekty do realizacji:\n",
    "1. Analiza sentymentu recenzji\n",
    "2. Klasyfikacja tekstów (spam detection)\n",
    "3. Eksploracja korpusu tekstowego\n",
    "4. Mini-projekt: Budowa klasyfikatora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import wszystkich potrzebnych bibliotek\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# Konfiguracja\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Pobierz zasoby NLTK\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"✅ Biblioteki załadowane!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekt 1: Analiza Sentymentu Recenzji\n",
    "\n",
    "### Cel:\n",
    "Przeanalizuj recenzje produktów i określ ich sentyment (pozytywny/negatywny)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stwórzmy zbiór przykładowych recenzji\n",
    "recenzje = [\n",
    "    (\"Świetny produkt! Bardzo jestem zadowolony. Polecam!\", \"pozytywny\"),\n",
    "    (\"Okropne. Nie działa tak jak powinno. Zwracam.\", \"negatywny\"),\n",
    "    (\"Całkiem niezły, ale cena mogłaby być niższa.\", \"neutralny\"),\n",
    "    (\"Najlepszy zakup w tym roku! Fantastyczna jakość!\", \"pozytywny\"),\n",
    "    (\"Totalna porażka. Strata pieniędzy.\", \"negatywny\"),\n",
    "    (\"Produkt jest OK, nic specjalnego.\", \"neutralny\"),\n",
    "    (\"Rewelacja! Dokładnie to czego szukałem!\", \"pozytywny\"),\n",
    "    (\"Nie polecam. Słaba jakość wykonania.\", \"negatywny\"),\n",
    "    (\"Za tę cenę spodziewałem się więcej.\", \"negatywny\"),\n",
    "    (\"Solidny produkt. Spełnia oczekiwania.\", \"pozytywny\"),\n",
    "]\n",
    "\n",
    "# Utwórz DataFrame\n",
    "df = pd.DataFrame(recenzje, columns=['tekst', 'sentyment'])\n",
    "\n",
    "print(\"=== ZBIÓR DANYCH ===\")\n",
    "print(df)\n",
    "print(f\"\\nRozkład klas:\")\n",
    "print(df['sentyment'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analiza eksploracyjna\n",
    "print(\"\\n=== STATYSTYKI PODSTAWOWE ===\")\n",
    "\n",
    "# Długość tekstów\n",
    "df['dlugosc'] = df['tekst'].apply(len)\n",
    "df['liczba_slow'] = df['tekst'].apply(lambda x: len(x.split()))\n",
    "\n",
    "print(df[['tekst', 'sentyment', 'dlugosc', 'liczba_slow']].head())\n",
    "\n",
    "# Statystyki\n",
    "print(\"\\nŚrednia długość tekstu:\", df['dlugosc'].mean())\n",
    "print(\"Średnia liczba słów:\", df['liczba_slow'].mean())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Wizualizacja rozkładu sentymentu\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "df['sentyment'].value_counts().plot(kind='bar')\n",
    "plt.title('Rozkład sentymentu')\n",
    "plt.xlabel('Sentyment')\n",
    "plt.ylabel('Liczba recenzji')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "df.groupby('sentyment')['liczba_slow'].mean().plot(kind='bar', color='coral')\n",
    "plt.title('Średnia liczba słów według sentymentu')\n",
    "plt.xlabel('Sentyment')\n",
    "plt.ylabel('Średnia liczba słów')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analiza najczęstszych słów\n",
    "from collections import Counter\n",
    "\n",
    "# Tokenizacja i liczenie słów\n",
    "wszystkie_slowa = []\n",
    "stop_words = set(stopwords.words('polish'))\n",
    "\n",
    "for tekst in df['tekst']:\n",
    "    tokens = word_tokenize(tekst.lower(), language='polish')\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    wszystkie_slowa.extend(tokens)\n",
    "\n",
    "# Najczęstsze słowa\n",
    "najczestsze = Counter(wszystkie_slowa).most_common(15)\n",
    "\n",
    "print(\"\\n=== 15 NAJCZĘSTSZYCH SŁÓW ===\")\n",
    "for slowo, liczba in najczestsze:\n",
    "    print(f\"{slowo:20} : {liczba}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# WordCloud - chmura słów\n",
    "try:\n",
    "    tekst_caly = ' '.join(wszystkie_slowa)\n",
    "    \n",
    "    wordcloud = WordCloud(width=800, height=400, \n",
    "                          background_color='white',\n",
    "                          colormap='viridis').generate(tekst_caly)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Chmura słów z recenzji', fontsize=16)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Błąd generowania chmury słów: {e}\")\n",
    "    print(\"Zainstaluj: pip install wordcloud\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekt 2: Klasyfikacja Spam vs Ham (SMS)\n",
    "\n",
    "### Cel:\n",
    "Zbuduj prosty klasyfikator do rozróżniania spamu od prawidłowych wiadomości."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przykładowy dataset SMS\n",
    "sms_data = [\n",
    "    (\"Wygrałeś 1000 zł! Kliknij link aby odebrać nagrodę!\", \"spam\"),\n",
    "    (\"Cześć, spotkamy się jutro o 18?\", \"ham\"),\n",
    "    (\"GRATULACJE! Zostałeś wybrany do otrzymania iPhone'a!\", \"spam\"),\n",
    "    (\"Pamiętaj o spotkaniu z klientem w czwartek\", \"ham\"),\n",
    "    (\"Kup teraz! Wielka promocja tylko dziś! -90%\", \"spam\"),\n",
    "    (\"Dzięki za wczorajsze spotkanie. Pozdrawiam\", \"ham\"),\n",
    "    (\"Pilne! Twoje konto zostało zablokowane. Kliknij tutaj\", \"spam\"),\n",
    "    (\"Mama dzwoniła, oddzwoń jak będziesz miał czas\", \"ham\"),\n",
    "    (\"DARMOWE PIENIĄDZE! Nie czekaj, zarejestruj się!\", \"spam\"),\n",
    "    (\"Kupiłem mleko, wracam za godzinę\", \"ham\"),\n",
    "    (\"Ostatnia szansa! Oferta wygasa za 2 godziny!!!\", \"spam\"),\n",
    "    (\"Spotkanie przeniesione na piątek o 14:00\", \"ham\"),\n",
    "]\n",
    "\n",
    "df_sms = pd.DataFrame(sms_data, columns=['wiadomosc', 'kategoria'])\n",
    "\n",
    "print(\"=== DATASET SMS ===\")\n",
    "print(df_sms)\n",
    "print(f\"\\nRozkład klas:\")\n",
    "print(df_sms['kategoria'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Preprocessing - pipeline\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Przetwarzanie tekstu przed klasyfikacją.\n",
    "    \"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenizacja\n",
    "    tokens = word_tokenize(text, language='polish')\n",
    "    \n",
    "    # Usuń znaki specjalne i liczby\n",
    "    tokens = [t for t in tokens if t.isalpha()]\n",
    "    \n",
    "    # Usuń stop words\n",
    "    stop_words = set(stopwords.words('polish'))\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Zastosuj preprocessing\n",
    "df_sms['wiadomosc_przetworzona'] = df_sms['wiadomosc'].apply(preprocess_text)\n",
    "\n",
    "print(\"=== PRZYKŁADY PRZETWARZANIA ===\")\n",
    "for idx in [0, 1, 4]:\n",
    "    print(f\"\\nOryginał: {df_sms.iloc[idx]['wiadomosc']}\")\n",
    "    print(f\"Przetworzone: {df_sms.iloc[idx]['wiadomosc_przetworzona']}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Feature Engineering - TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=50)\n",
    "\n",
    "X = vectorizer.fit_transform(df_sms['wiadomosc_przetworzona'])\n",
    "y = df_sms['kategoria']\n",
    "\n",
    "print(\"=== FEATURE ENGINEERING ===\")\n",
    "print(f\"Shape macierzy cech: {X.shape}\")\n",
    "print(f\"\\nNajważniejsze cechy (słowa):\")\n",
    "print(vectorizer.get_feature_names_out()[:20])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Podział na zbiór treningowy i testowy\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(\"=== PODZIAŁ DANYCH ===\")\n",
    "print(f\"Zbiór treningowy: {X_train.shape[0]} próbek\")\n",
    "print(f\"Zbiór testowy: {X_test.shape[0]} próbek\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Trening modelu - Naive Bayes\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predykcja\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"=== WYNIKI MODELU ===\")\n",
    "print(f\"\\nDokładność: {accuracy_score(y_test, y_pred):.2%}\")\n",
    "print(\"\\nRaport klasyfikacji:\")\n",
    "print(classification_report(y_test, y_pred))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Test modelu na nowych danych\n",
    "nowe_wiadomosci = [\n",
    "    \"Wygrałeś milion złotych! Kliknij teraz!\",\n",
    "    \"Spotkamy się w kawiarni o 16?\",\n",
    "    \"PROMOCJA! Kup teraz ze zniżką 99%\",\n",
    "]\n",
    "\n",
    "print(\"\\n=== TEST NA NOWYCH DANYCH ===\")\n",
    "for wiadomosc in nowe_wiadomosci:\n",
    "    przetworzona = preprocess_text(wiadomosc)\n",
    "    wektor = vectorizer.transform([przetworzona])\n",
    "    predykcja = model.predict(wektor)[0]\n",
    "    prawdopodobienstwo = model.predict_proba(wektor)[0]\n",
    "    \n",
    "    print(f\"\\nWiadomość: {wiadomosc}\")\n",
    "    print(f\"Predykcja: {predykcja}\")\n",
    "    print(f\"Prawdopodobieństwo: ham={prawdopodobienstwo[0]:.2%}, spam={prawdopodobienstwo[1]:.2%}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projekt 3: Eksploracja korpusu tekstowego\n",
    "\n",
    "### Cel:\n",
    "Przeanalizuj większy zbiór tekstów i wyciągnij interesujące wnioski."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Utwórz większy korpus (symulacja artykułów)\n",
    "artykuly = [\n",
    "    {\n",
    "        'tytul': 'Sztuczna inteligencja w medycynie',\n",
    "        'tekst': 'Sztuczna inteligencja rewolucjonizuje medycynę. Algorytmy uczenia maszynowego pomagają w diagnostyce chorób. Systemy AI analizują obrazy medyczne z dokładnością przewyższającą ludzkich ekspertów.',\n",
    "        'kategoria': 'technologia'\n",
    "    },\n",
    "    {\n",
    "        'tytul': 'Nowe odkrycie archeologiczne',\n",
    "        'tekst': 'Archeolodzy odkryli starożytne miasto. Znaleziska obejmują ceramikę, narzędzia i ruiny budynków. Odkrycie rzuca nowe światło na historię starożytnych cywilizacji.',\n",
    "        'kategoria': 'historia'\n",
    "    },\n",
    "    {\n",
    "        'tytul': 'Przyszłość transportu',\n",
    "        'tekst': 'Pojazdy autonomiczne to przyszłość transportu. Samochody bez kierowcy będą bezpieczniejsze i bardziej efektywne. Technologia ta zmieni sposób w jaki się przemieszczamy.',\n",
    "        'kategoria': 'technologia'\n",
    "    },\n",
    "    {\n",
    "        'tytul': 'Zmiany klimatyczne',\n",
    "        'tekst': 'Globalne ocieplenie wpływa na ekosystemy. Naukowcy ostrzegają przed konsekwencjami zmian klimatycznych. Konieczne są pilne działania aby chronić naszą planetę.',\n",
    "        'kategoria': 'środowisko'\n",
    "    },\n",
    "]\n",
    "\n",
    "df_artykuly = pd.DataFrame(artykuly)\n",
    "print(\"=== KORPUS ARTYKUŁÓW ===\")\n",
    "print(df_artykuly[['tytul', 'kategoria']])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Analiza z użyciem spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"pl_core_news_sm\")\n",
    "except:\n",
    "    print(\"⚠️ Zainstaluj model polski: python -m spacy download pl_core_news_sm\")\n",
    "    nlp = None\n",
    "\n",
    "if nlp:\n",
    "    print(\"\\n=== ANALIZA LINGUISTYCZNA ===\")\n",
    "    \n",
    "    for idx, row in df_artykuly.iterrows():\n",
    "        doc = nlp(row['tekst'])\n",
    "        \n",
    "        print(f\"\\n{row['tytul']}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Rzeczowniki\n",
    "        rzeczowniki = [token.lemma_ for token in doc if token.pos_ == 'NOUN']\n",
    "        print(f\"Kluczowe rzeczowniki: {list(set(rzeczowniki))[:5]}\")\n",
    "        \n",
    "        # Nazwane encje\n",
    "        if doc.ents:\n",
    "            encje = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "            print(f\"Encje: {encje}\")\n",
    "        \n",
    "        # Statystyki\n",
    "        print(f\"Liczba zdań: {len(list(doc.sents))}\")\n",
    "        print(f\"Liczba tokenów: {len(doc)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie finalne: Mini-projekt\n",
    "\n",
    "### Zadanie:\n",
    "Stwórz kompletny pipeline klasyfikacji tekstu:\n",
    "1. Załaduj lub stwórz dane\n",
    "2. Wykonaj eksploracyjną analizę danych (EDA)\n",
    "3. Przetwórz tekst (preprocessing)\n",
    "4. Wyekstraktuj cechy (TF-IDF lub inne)\n",
    "5. Wytrenuj model (wybierz algorytm)\n",
    "6. Oceń model (metryki, confusion matrix)\n",
    "7. Przetestuj na nowych danych\n",
    "\n",
    "### Wskazówki:\n",
    "- Używaj poznanych technik z całego dnia\n",
    "- Eksperymentuj z różnymi parametrami\n",
    "- Porównaj różne modele (Naive Bayes, Logistic Regression)\n",
    "- Wizualizuj wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# TWOJA IMPLEMENTACJA\n",
    "# Rozpocznij tutaj swoją pracę nad mini-projektem\n",
    "\n",
    "# Przykładowa struktura:\n",
    "\n",
    "# 1. Załaduj dane\n",
    "# data = ...\n",
    "\n",
    "# 2. EDA\n",
    "# ...\n",
    "\n",
    "# 3. Preprocessing\n",
    "# ...\n",
    "\n",
    "# 4. Feature extraction\n",
    "# ...\n",
    "\n",
    "# 5. Model training\n",
    "# ...\n",
    "\n",
    "# 6. Evaluation\n",
    "# ...\n",
    "\n",
    "# 7. Testing\n",
    "# ...\n",
    "\n",
    "print(\"🚀 Powodzenia w realizacji projektu!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodatkowe wyzwania (opcjonalne)\n",
    "\n",
    "### Wyzwanie 1: Porównanie modeli\n",
    "Porównaj wydajność różnych algorytmów:\n",
    "- Naive Bayes\n",
    "- Logistic Regression\n",
    "- SVM\n",
    "- Random Forest\n",
    "\n",
    "### Wyzwanie 2: Feature engineering\n",
    "Spróbuj różnych metod ekstrakcji cech:\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "- N-gramy (bigrams, trigrams)\n",
    "- Word embeddings\n",
    "\n",
    "### Wyzwanie 3: Optymalizacja\n",
    "Użyj GridSearchCV do znalezienia najlepszych hiperparametrów.\n",
    "\n",
    "### Wyzwanie 4: Analiza błędów\n",
    "Przeanalizuj przypadki w których model się myli - co można poprawić?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie Dnia 1\n",
    "\n",
    "### Czego się nauczyliśmy:\n",
    "✅ Podstawy NLP i jego zastosowań\n",
    "\n",
    "✅ Narzędzia: NLTK, spaCy, Hugging Face, OpenAI\n",
    "\n",
    "✅ Operacje: tokenizacja, lematyzacja, POS tagging\n",
    "\n",
    "✅ Praktyczne projekty: analiza sentymentu, spam detection\n",
    "\n",
    "✅ Pipeline ML: od danych do modelu\n",
    "\n",
    "### Na jutro:\n",
    "🚀 Nowoczesne modele NLP (Transformery)\n",
    "\n",
    "🚀 BERT, GPT i fine-tuning\n",
    "\n",
    "🚀 Generowanie i rozumienie tekstu\n",
    "\n",
    "🚀 Zastosowania biznesowe (chatboty, automatyzacja)\n",
    "\n",
    "---\n",
    "\n",
    "**Dobra robota! 🎉 Do zobaczenia jutro!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
