{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dzie≈Ñ 1 - Modu≈Ç 2: Narzƒôdzia i biblioteki do NLP\n",
    "\n",
    "## Cele modu≈Çu:\n",
    "- Poznanie najpopularniejszych bibliotek NLP w Pythonie\n",
    "- Instalacja i konfiguracja narzƒôdzi\n",
    "- Praktyczne por√≥wnanie NLTK, spaCy i Hugging Face\n",
    "- Integracja z OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 PrzeglƒÖd ekosystemu bibliotek NLP w Pythonie\n",
    "\n",
    "### G≈Ç√≥wne biblioteki:\n",
    "\n",
    "| Biblioteka | Zastosowanie | Poziom | Szybko≈õƒá |\n",
    "|------------|--------------|--------|----------|\n",
    "| **NLTK** | Edukacja, prototypy | PoczƒÖtkujƒÖcy | Wolna |\n",
    "| **spaCy** | Produkcja, pipeline'y | ≈öredniozaawansowany | Szybka |\n",
    "| **Hugging Face** | Transformery, ML | Zaawansowany | Zale≈ºna od modelu |\n",
    "| **TextBlob** | Proste analizy | PoczƒÖtkujƒÖcy | ≈örednia |\n",
    "| **Gensim** | Topic modeling | ≈öredniozaawansowany | ≈örednia |\n",
    "| **OpenAI API** | LLM, generowanie | Wszystkie | Szybka |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 NLTK (Natural Language Toolkit)\n",
    "\n",
    "### Charakterystyka:\n",
    "- üìö Najbardziej edukacyjna biblioteka\n",
    "- üß∞ Ponad 50 korpus√≥w i zasob√≥w leksykalnych\n",
    "- üéì ≈öwietna do nauki podstaw NLP\n",
    "- ‚ö†Ô∏è Wolniejsza ni≈º spaCy\n",
    "\n",
    "### Instalacja:\n",
    "```bash\n",
    "pip install nltk\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Instalacja i pobieranie zasob√≥w NLTK\n",
    "import nltk\n",
    "\n",
    "# Pobierz podstawowe zasoby (wykonaj raz)\n",
    "resources = ['punkt', 'stopwords', 'averaged_perceptron_tagger', 'wordnet', 'omw-1.4']\n",
    "\n",
    "for resource in resources:\n",
    "    try:\n",
    "        nltk.download(resource, quiet=True)\n",
    "    except:\n",
    "        print(f\"Nie mo≈ºna pobraƒá: {resource}\")\n",
    "\n",
    "print(\"‚úÖ NLTK zasoby pobrane!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad u≈ºycia NLTK - tokenizacja\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tekst = \"\"\"\n",
    "Natural Language Processing to fascynujƒÖca dziedzina sztucznej inteligencji.\n",
    "Pozwala komputerom rozumieƒá i generowaƒá ludzki jƒôzyk.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenizacja zda≈Ñ\n",
    "zdania = sent_tokenize(tekst)\n",
    "print(\"=== Zdania ===\")\n",
    "for i, zdanie in enumerate(zdania, 1):\n",
    "    print(f\"{i}. {zdanie.strip()}\")\n",
    "\n",
    "print(\"\\n=== S≈Çowa ===\")\n",
    "# Tokenizacja s≈Ç√≥w\n",
    "slowa = word_tokenize(tekst)\n",
    "print(slowa[:15])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# NLTK - Part of Speech Tagging (dla angielskiego)\n",
    "from nltk import pos_tag\n",
    "\n",
    "tekst_en = \"The quick brown fox jumps over the lazy dog.\"\n",
    "slowa_en = word_tokenize(tekst_en)\n",
    "tagged = pos_tag(slowa_en)\n",
    "\n",
    "print(\"=== POS Tagging (angielski) ===\")\n",
    "for slowo, tag in tagged:\n",
    "    print(f\"{slowo:15} -> {tag}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 spaCy - Produkcyjna biblioteka NLP\n",
    "\n",
    "### Charakterystyka:\n",
    "- üöÄ Bardzo szybka (zoptymalizowana w Cython)\n",
    "- üè≠ Gotowa do u≈ºycia w produkcji\n",
    "- üåç Wsparcie dla wielu jƒôzyk√≥w (polski te≈º!)\n",
    "- üéØ Pipeline'y do konkretnych zada≈Ñ\n",
    "\n",
    "### Instalacja:\n",
    "```bash\n",
    "pip install spacy\n",
    "python -m spacy download pl_core_news_sm  # model polski\n",
    "python -m spacy download en_core_web_sm   # model angielski\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad u≈ºycia spaCy\n",
    "import spacy\n",
    "\n",
    "# Za≈Çaduj model polski (je≈õli jest zainstalowany)\n",
    "try:\n",
    "    nlp = spacy.load(\"pl_core_news_sm\")\n",
    "    tekst = \"Jan Kowalski pracuje w Warszawie dla firmy Microsoft.\"\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Model polski nie jest zainstalowany. U≈ºywam angielskiego.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    tekst = \"John Smith works in New York for Microsoft Corporation.\"\n",
    "\n",
    "# Przetw√≥rz tekst\n",
    "doc = nlp(tekst)\n",
    "\n",
    "print(\"=== Tokeny i ich w≈Ça≈õciwo≈õci ===\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} | Lemma: {token.lemma_:15} | POS: {token.pos_:8} | Tag: {token.tag_}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# spaCy - Named Entity Recognition (NER)\n",
    "print(\"\\n=== Rozpoznawanie Nazwanych Encji (NER) ===\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:20} -> {ent.label_:10} ({spacy.explain(ent.label_)})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# spaCy - Dependency Parsing (analiza zale≈ºno≈õci)\n",
    "print(\"\\n=== Analiza zale≈ºno≈õci sk≈Çadniowych ===\")\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:15} <--[{token.dep_:10}]-- {token.head.text}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# spaCy - Podobie≈Ñstwo semantyczne\n",
    "try:\n",
    "    # Wymaga modelu z wektorami (np. en_core_web_md lub pl_core_news_md)\n",
    "    doc1 = nlp(\"kot\")\n",
    "    doc2 = nlp(\"pies\")\n",
    "    doc3 = nlp(\"samoch√≥d\")\n",
    "    \n",
    "    print(\"\\n=== Podobie≈Ñstwo semantyczne ===\")\n",
    "    print(f\"Podobie≈Ñstwo 'kot' <-> 'pies': {doc1.similarity(doc2):.3f}\")\n",
    "    print(f\"Podobie≈Ñstwo 'kot' <-> 'samoch√≥d': {doc1.similarity(doc3):.3f}\")\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è Podobie≈Ñstwo wymaga modelu z wektorami (np. en_core_web_md)\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Hugging Face Transformers - Nowoczesne modele NLP\n",
    "\n",
    "### Charakterystyka:\n",
    "- ü§ó Najwiƒôksza biblioteka modeli Transformer\n",
    "- üöÄ TysiƒÖce pre-trenowanych modeli\n",
    "- üåç Wsparcie dla 100+ jƒôzyk√≥w\n",
    "- üî• PyTorch i TensorFlow\n",
    "- üì¶ Model Hub - ≈Çatwe pobieranie i dzielenie siƒô modelami\n",
    "\n",
    "### Instalacja:\n",
    "```bash\n",
    "pip install transformers torch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad: Pipeline'y w Hugging Face\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"=== Pipeline do analizy sentymentu ===\")\n",
    "\n",
    "# Automatycznie pobiera i u≈ºywa odpowiedniego modelu\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "teksty = [\n",
    "    \"I love this product!\",\n",
    "    \"This is terrible and disappointing.\",\n",
    "    \"It's okay, nothing special.\"\n",
    "]\n",
    "\n",
    "wyniki = classifier(teksty)\n",
    "\n",
    "for tekst, wynik in zip(teksty, wyniki):\n",
    "    print(f\"Tekst: {tekst}\")\n",
    "    print(f\"  -> {wynik['label']}: {wynik['score']:.3f}\")\n",
    "    print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pipeline do rozpoznawania nazwanych encji\n",
    "print(\"=== Named Entity Recognition ===\")\n",
    "\n",
    "ner = pipeline(\"ner\", grouped_entities=True)\n",
    "\n",
    "tekst = \"Elon Musk is the CEO of Tesla and SpaceX, based in California.\"\n",
    "entities = ner(tekst)\n",
    "\n",
    "for entity in entities:\n",
    "    print(f\"{entity['word']:20} -> {entity['entity_group']:10} (score: {entity['score']:.3f})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pipeline do podsumowywania tekstu\n",
    "print(\"\\n=== Podsumowanie tekstu ===\")\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "dlugi_tekst = \"\"\"\n",
    "Artificial intelligence has made remarkable progress in recent years. \n",
    "Machine learning models, particularly deep neural networks, have achieved \n",
    "superhuman performance in many tasks. Natural language processing has \n",
    "especially benefited from transformer architectures like BERT and GPT. \n",
    "These models can understand context, generate human-like text, and perform \n",
    "various NLP tasks with minimal fine-tuning. The impact on industries \n",
    "ranging from healthcare to finance has been profound.\n",
    "\"\"\"\n",
    "\n",
    "streszczenie = summarizer(dlugi_tekst, max_length=50, min_length=25, do_sample=False)\n",
    "print(\"Orygina≈Ç:\", len(dlugi_tekst.split()), \"s≈Ç√≥w\")\n",
    "print(\"Streszczenie:\", streszczenie[0]['summary_text'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 OpenAI API - Large Language Models\n",
    "\n",
    "### Charakterystyka:\n",
    "- üß† Najpotƒô≈ºniejsze modele jƒôzykowe (GPT-4, GPT-3.5)\n",
    "- ‚òÅÔ∏è API w chmurze - brak potrzeby w≈Çasnej infrastruktury\n",
    "- üí∞ P≈Çatne (ale tanie dla ma≈Çych projekt√≥w)\n",
    "- üéØ Uniwersalne - jedna API do wielu zada≈Ñ\n",
    "\n",
    "### Instalacja:\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "\n",
    "### Konfiguracja:\n",
    "Potrzebujesz klucza API z https://platform.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad u≈ºycia OpenAI API\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# UWAGA: Ustaw sw√≥j klucz API\n",
    "# Spos√≥b 1: Zmienna ≈õrodowiskowa (zalecane)\n",
    "# export OPENAI_API_KEY='your-api-key'\n",
    "\n",
    "# Spos√≥b 2: Bezpo≈õrednio (NIE commituj do repozytorium!)\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-api-key'\n",
    "\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Przyk≈Çad: Analiza sentymentu\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Jeste≈õ ekspertem od analizy sentymentu. Odpowiadaj kr√≥tko: POZYTYWNY, NEGATYWNY lub NEUTRALNY.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Oce≈Ñ sentyment: Ten produkt jest fantastyczny! Bardzo polecam.\"}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    print(\"=== OpenAI API - Analiza sentymentu ===\")\n",
    "    print(\"Odpowied≈∫:\", response.choices[0].message.content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è B≈ÇƒÖd: {e}\")\n",
    "    print(\"Upewnij siƒô, ≈ºe masz ustawiony klucz API: OPENAI_API_KEY\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przyk≈Çad: Ekstrakcja informacji z tekstu\n",
    "try:\n",
    "    tekst_do_analizy = \"\"\"\n",
    "    Firma TechCorp og≈Çosi≈Ça nowƒÖ inwestycjƒô w wysoko≈õci 50 milion√≥w z≈Çotych. \n",
    "    CEO Jan Kowalski poinformowa≈Ç, ≈ºe pieniƒÖdze zostanƒÖ przeznaczone na rozw√≥j \n",
    "    sztucznej inteligencji. Projekt ma ruszyƒá w maju 2024 roku.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"WyciƒÖgnij z tekstu: firmƒô, osobƒô, kwotƒô i datƒô. Odpowiedz w formacie JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": tekst_do_analizy}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== OpenAI API - Ekstrakcja informacji ===\")\n",
    "    print(response.choices[0].message.content)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è OpenAI API niedostƒôpne: {e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Por√≥wnanie bibliotek - Kiedy u≈ºywaƒá kt√≥rej?\n",
    "\n",
    "### NLTK - U≈ºywaj gdy:\n",
    "- ‚úÖ Uczysz siƒô NLP\n",
    "- ‚úÖ Potrzebujesz podstawowych operacji\n",
    "- ‚úÖ Pracujesz z korpusami lingwistycznymi\n",
    "- ‚úÖ Robisz proste prototypy\n",
    "\n",
    "### spaCy - U≈ºywaj gdy:\n",
    "- ‚úÖ Budujesz aplikacjƒô produkcyjnƒÖ\n",
    "- ‚úÖ Potrzebujesz szybko≈õci\n",
    "- ‚úÖ Pracujesz z pipeline'ami NLP\n",
    "- ‚úÖ Chcesz gotowe rozwiƒÖzania (NER, POS, itp.)\n",
    "\n",
    "### Hugging Face - U≈ºywaj gdy:\n",
    "- ‚úÖ Potrzebujesz najnowszych modeli Transformer\n",
    "- ‚úÖ Pracujesz z wieloma jƒôzykami\n",
    "- ‚úÖ Fine-tuningujesz modele\n",
    "- ‚úÖ Chcesz dostƒôp do tysiƒôcy pre-trenowanych modeli\n",
    "\n",
    "### OpenAI API - U≈ºywaj gdy:\n",
    "- ‚úÖ Potrzebujesz najlepszej jako≈õci\n",
    "- ‚úÖ Nie chcesz siƒô martwiƒá infrastrukturƒÖ\n",
    "- ‚úÖ Zadanie wymaga rozumienia kontekstu\n",
    "- ‚úÖ Bud≈ºet pozwala na API calls\n",
    "- ‚ö†Ô∏è Pamiƒôtaj o prywatno≈õci danych!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ƒÜwiczenie praktyczne\n",
    "\n",
    "**Zadanie**: Wykonaj to samo zadanie NLP u≈ºywajƒÖc r√≥≈ºnych bibliotek\n",
    "\n",
    "Zadanie: Znajd≈∫ wszystkie osoby i lokalizacje w tek≈õcie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "tekst_testowy = \"\"\"\n",
    "Adam Mickiewicz urodzi≈Ç siƒô w Zaosiu niedaleko Nowogr√≥dka. \n",
    "Studiowa≈Ç na Uniwersytecie Wile≈Ñskim. P√≥≈∫niej przeni√≥s≈Ç siƒô do Pary≈ºa, \n",
    "gdzie pozna≈Ç Juliusza S≈Çowackiego.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== TEKST DO ANALIZY ===\")\n",
    "print(tekst_testowy)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# TODO: Spr√≥buj wyciƒÖgnƒÖƒá nazwane encje u≈ºywajƒÖc:\n",
    "# 1. spaCy\n",
    "# 2. Hugging Face\n",
    "# 3. OpenAI API\n",
    "\n",
    "# Kt√≥ry spos√≥b dzia≈Ça najlepiej?\n",
    "# Kt√≥ry jest najszybszy?\n",
    "# Kt√≥ry jest naj≈Çatwiejszy w u≈ºyciu?"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# RozwiƒÖzanie 1: spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"pl_core_news_sm\")\n",
    "    doc = nlp(tekst_testowy)\n",
    "    \n",
    "    print(\"\\n=== ROZWIƒÑZANIE: spaCy ===\")\n",
    "    osoby = [ent.text for ent in doc.ents if ent.label_ == \"persName\"]\n",
    "    miejsca = [ent.text for ent in doc.ents if ent.label_ in [\"placeName\", \"geogName\"]]\n",
    "    \n",
    "    print(f\"Osoby: {osoby}\")\n",
    "    print(f\"Miejsca: {miejsca}\")\n",
    "except Exception as e:\n",
    "    print(f\"spaCy: {e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# RozwiƒÖzanie 2: Hugging Face\n",
    "try:\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", grouped_entities=True)\n",
    "    entities = ner_pipeline(tekst_testowy)\n",
    "    \n",
    "    print(\"\\n=== ROZWIƒÑZANIE: Hugging Face ===\")\n",
    "    osoby = [e['word'] for e in entities if e['entity_group'] == 'PER']\n",
    "    miejsca = [e['word'] for e in entities if e['entity_group'] == 'LOC']\n",
    "    \n",
    "    print(f\"Osoby: {osoby}\")\n",
    "    print(f\"Miejsca: {miejsca}\")\n",
    "except Exception as e:\n",
    "    print(f\"Hugging Face: {e}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie Modu≈Çu 2\n",
    "\n",
    "‚úÖ Poznali≈õmy g≈Ç√≥wne biblioteki NLP (NLTK, spaCy, Hugging Face, OpenAI)\n",
    "\n",
    "‚úÖ Nauczyli≈õmy siƒô instalowaƒá i konfigurowaƒá narzƒôdzia\n",
    "\n",
    "‚úÖ Wykonali≈õmy praktyczne przyk≈Çady z ka≈ºdej biblioteki\n",
    "\n",
    "‚úÖ Wiemy kiedy u≈ºywaƒá kt√≥rej biblioteki\n",
    "\n",
    "---\n",
    "\n",
    "**Nastƒôpny krok**: Modu≈Ç 3 - Podstawowe operacje w NLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
