{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dzień 1 - Moduł 1: Wprowadzenie do NLP i jego zastosowań\n",
    "\n",
    "## Cele modułu:\n",
    "- Zrozumienie czym jest przetwarzanie języka naturalnego (NLP)\n",
    "- Poznanie głównych zastosowań NLP w praktyce\n",
    "- Porównanie klasycznych i nowoczesnych podejść do NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Czym jest NLP (Natural Language Processing)?\n",
    "\n",
    "**Natural Language Processing (NLP)** to dziedzina sztucznej inteligencji, która zajmuje się interakcją między komputerami a językiem ludzkim.\n",
    "\n",
    "### Główne cele NLP:\n",
    "- **Rozumienie** - analiza i interpretacja tekstu\n",
    "- **Generowanie** - tworzenie naturalnie brzmiącego tekstu\n",
    "- **Przekształcanie** - tłumaczenia, podsumowania, parafrazy\n",
    "\n",
    "### Wyzwania w NLP:\n",
    "- Wieloznaczność (ambiguity)\n",
    "- Kontekst i znaczenie ukryte\n",
    "- Idiomy i wyrażenia kulturowe\n",
    "- Zróżnicowanie językowe (dialekty, slang)\n",
    "- Błędy ortograficzne i gramatyczne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Przegląd zastosowań NLP\n",
    "\n",
    "### 1. Chatboty i Asystenci Wirtualni\n",
    "- Obsługa klienta 24/7\n",
    "- Automatyczne odpowiedzi na FAQ\n",
    "- Rezerwacje i zamówienia\n",
    "- Przykłady: ChatGPT, Alexa, Siri, Google Assistant\n",
    "\n",
    "### 2. Analiza Sentymentu (Sentiment Analysis)\n",
    "- Analiza opinii klientów\n",
    "- Monitoring marki w social media\n",
    "- Analiza nastrojów na rynku\n",
    "- Ocena produktów i usług\n",
    "\n",
    "### 3. Tłumaczenia Maszynowe\n",
    "- Google Translate, DeepL\n",
    "- Tłumaczenia dokumentów\n",
    "- Tłumaczenia w czasie rzeczywistym\n",
    "\n",
    "### 4. Podsumowania Tekstu\n",
    "- Automatyczne streszczenia artykułów\n",
    "- Analiza dokumentów prawnych\n",
    "- Agregacja wiadomości\n",
    "\n",
    "### 5. Rozpoznawanie Nazwanych Encji (NER)\n",
    "- Ekstrakcja informacji z dokumentów\n",
    "- Identyfikacja osób, miejsc, organizacji\n",
    "- Analiza CV i ofert pracy\n",
    "\n",
    "### 6. Systemy Pytań i Odpowiedzi (Q&A)\n",
    "- Wyszukiwanie informacji w bazach wiedzy\n",
    "- Systemy help desk\n",
    "- Edukacyjne platformy e-learningowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Prosty przykład - analiza sentymentu z użyciem biblioteki TextBlob\n",
    "# (instalacja: pip install textblob)\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Przykładowe teksty w języku angielskim\n",
    "teksty = [\n",
    "    \"I love this product! It's amazing and works perfectly.\",\n",
    "    \"This is the worst experience I've ever had. Terrible!\",\n",
    "    \"The product is okay. Nothing special but does the job.\"\n",
    "]\n",
    "\n",
    "for tekst in teksty:\n",
    "    analiza = TextBlob(tekst)\n",
    "    sentiment = analiza.sentiment\n",
    "    \n",
    "    # Polarity: -1 (negatywny) do 1 (pozytywny)\n",
    "    if sentiment.polarity > 0.1:\n",
    "        kategoria = \"POZYTYWNY\"\n",
    "    elif sentiment.polarity < -0.1:\n",
    "        kategoria = \"NEGATYWNY\"\n",
    "    else:\n",
    "        kategoria = \"NEUTRALNY\"\n",
    "    \n",
    "    print(f\"Tekst: {tekst}\")\n",
    "    print(f\"Sentyment: {kategoria} (polarity: {sentiment.polarity:.2f})\")\n",
    "    print(\"-\" * 80)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Modele NLP: Klasyczne podejścia vs Nowoczesne architektury\n",
    "\n",
    "### Klasyczne podejścia (do ~2017)\n",
    "\n",
    "#### 1. Metody oparte na regułach\n",
    "- Ręcznie tworzone reguły gramatyczne\n",
    "- Słowniki i wzorce\n",
    "- **Zalety**: Przewidywalne, łatwe do debugowania\n",
    "- **Wady**: Trudne w skalowaniu, wymagają ekspertów\n",
    "\n",
    "#### 2. Bag of Words (BoW) i TF-IDF\n",
    "- Reprezentacja tekstu jako zbioru słów\n",
    "- Utrata informacji o kolejności\n",
    "- **Zalety**: Proste, szybkie\n",
    "- **Wady**: Brak kontekstu, duża wymiarowość\n",
    "\n",
    "#### 3. N-gramy\n",
    "- Sekwencje n kolejnych słów\n",
    "- Częściowe zachowanie kontekstu\n",
    "- **Zalety**: Proste, działają lokalnie\n",
    "- **Wady**: Eksplozja wymiarowości\n",
    "\n",
    "#### 4. Word2Vec, GloVe (Word Embeddings)\n",
    "- Reprezentacja słów jako wektorów\n",
    "- Podobieństwo semantyczne\n",
    "- **Zalety**: Znaczenie słów, podobieństwo\n",
    "- **Wady**: Jedno znaczenie na słowo\n",
    "\n",
    "### Nowoczesne architektury (od ~2017)\n",
    "\n",
    "#### 1. Architektury RNN/LSTM\n",
    "- Przetwarzanie sekwencyjne\n",
    "- Pamięć kontekstu\n",
    "- **Wady**: Wolne, problem z długimi sekwencjami\n",
    "\n",
    "#### 2. Transformery (2017 - Attention is All You Need)\n",
    "- Mechanizm uwagi (attention)\n",
    "- Przetwarzanie równoległe\n",
    "- **Zalety**: Szybkie, skalowalne, potężne\n",
    "\n",
    "#### 3. BERT (2018)\n",
    "- Bidirectional Encoder Representations\n",
    "- Rozumienie kontekstu z obu stron\n",
    "- Pre-training + Fine-tuning\n",
    "\n",
    "#### 4. GPT (2018-2024)\n",
    "- Generative Pre-trained Transformer\n",
    "- Generowanie tekstu\n",
    "- GPT-3, GPT-4 - modele ogólnego przeznaczenia\n",
    "\n",
    "#### 5. T5, BART, inne modele Seq2Seq\n",
    "- Wszystko jako zadanie text-to-text\n",
    "- Uniwersalne podejście"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porównanie: Klasyczne vs Nowoczesne\n",
    "\n",
    "| Aspekt | Klasyczne | Nowoczesne (Transformery) |\n",
    "|--------|-----------|---------------------------|\n",
    "| Wydajność | Średnia | Wysoka |\n",
    "| Zrozumienie kontekstu | Ograniczone | Doskonałe |\n",
    "| Wymagania obliczeniowe | Niskie | Wysokie |\n",
    "| Ilość potrzebnych danych | Mała-Średnia | Duża (ale transfer learning!) |\n",
    "| Interpretowalność | Wysoka | Niska |\n",
    "| Czas treningu | Krótki | Długi |\n",
    "| Wszechstronność | Niska | Wysoka |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przykład: Porównanie klasycznego i nowoczesnego podejścia\n",
    "\n",
    "# KLASYCZNE: Bag of Words z sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "dokumenty = [\n",
    "    \"Lubię programować w Pythonie\",\n",
    "    \"Python to świetny język programowania\",\n",
    "    \"Uczę się sztucznej inteligencji\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(dokumenty)\n",
    "\n",
    "print(\"=== KLASYCZNE: Bag of Words ===\")\n",
    "print(\"Słownik:\", vectorizer.get_feature_names_out())\n",
    "print(\"Reprezentacja macierzowa:\")\n",
    "print(X_bow.toarray())\n",
    "print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# NOWOCZESNE: Embeddingi z Hugging Face\n",
    "# Uwaga: To tylko demonstracja - szczegóły w Module 2!\n",
    "\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import torch\n",
    "\n",
    "# # Załaduj model i tokenizer (HerBERT dla języka polskiego)\n",
    "# model_name = \"allegro/herbert-base-cased\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# tekst = \"Lubię programować w Pythonie\"\n",
    "\n",
    "# # Tokenizacja\n",
    "# inputs = tokenizer(tekst, return_tensors=\"pt\")\n",
    "\n",
    "# # Uzyskanie embeddingów\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(**inputs)\n",
    "#     embeddings = outputs.last_hidden_state\n",
    "\n",
    "# print(\"=== NOWOCZESNE: Transformer Embeddings ===\")\n",
    "# print(f\"Shape embeddingów: {embeddings.shape}\")\n",
    "# print(f\"Każde słowo reprezentowane przez wektor {embeddings.shape[-1]} wymiarów\")\n",
    "\n",
    "print(\"\\n📝 Nowoczesne modele przeanalizujemy szczegółowo w kolejnych modułach!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie praktyczne\n",
    "\n",
    "**Zadanie**: Zastanów się nad własnym projektem NLP\n",
    "\n",
    "1. Jakie zastosowanie NLP byłoby przydatne w Twojej pracy/firmie?\n",
    "2. Jakie dane tekstowe są dostępne?\n",
    "3. Czy potrzebujesz rozumienia czy generowania tekstu?\n",
    "4. Jakie metryki sukcesu byłyby ważne?\n",
    "\n",
    "Zapisz swoje przemyślenia poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Miejsce na Twoje notatki i pomysły\n",
    "\n",
    "moj_projekt = {\n",
    "    \"zastosowanie\": \"...\",\n",
    "    \"dostepne_dane\": \"...\",\n",
    "    \"typ_zadania\": \"...\",  # np. klasyfikacja, generowanie, NER, itp.\n",
    "    \"metryki\": \"...\"  # np. dokładność, precyzja, satysfakcja użytkowników\n",
    "}\n",
    "\n",
    "print(moj_projekt)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie Modułu 1\n",
    "\n",
    "✅ Poznaliśmy definicję i cele NLP\n",
    "\n",
    "✅ Przejrzeliśmy główne zastosowania (chatboty, sentiment, tłumaczenia, itp.)\n",
    "\n",
    "✅ Zrozumieliśmy ewolucję od klasycznych metod do Transformerów\n",
    "\n",
    "✅ Zobaczyliśmy proste przykłady w praktyce\n",
    "\n",
    "---\n",
    "\n",
    "**Następny krok**: Moduł 2 - Narzędzia i biblioteki do NLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
