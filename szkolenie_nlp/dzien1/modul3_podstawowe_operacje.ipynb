{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dzień 1 - Moduł 3: Podstawowe operacje w NLP\n",
    "\n",
    "## Cele modułu:\n",
    "- Tokenizacja tekstu na różne sposoby\n",
    "- Lematyzacja i stemming\n",
    "- Usuwanie stop words i normalizacja\n",
    "- POS tagging i dependency parsing\n",
    "- Praktyczne przetwarzanie struktur językowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import potrzebnych bibliotek\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# Pobierz niezbędne zasoby\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('omw-1.4', quiet=True)\n",
    "\n",
    "print(\"✅ Biblioteki załadowane!\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tokenizacja\n",
    "\n",
    "**Tokenizacja** to proces dzielenia tekstu na mniejsze jednostki (tokeny).\n",
    "\n",
    "### Rodzaje tokenizacji:\n",
    "- **Tokenizacja zdań** - podział na zdania\n",
    "- **Tokenizacja słów** - podział na słowa\n",
    "- **Tokenizacja podwyrazów** - podział na części słów (BPE, WordPiece)\n",
    "- **Tokenizacja znaków** - podział na pojedyncze znaki\n",
    "\n",
    "### Po co tokenizacja?\n",
    "- Przygotowanie danych do analizy\n",
    "- Podstawa dla wszystkich operacji NLP\n",
    "- Różne modele wymagają różnej tokenizacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Przykład tekstu\n",
    "tekst = \"\"\"\n",
    "Dr. Jan Kowalski pracuje w firmie AI-Tech sp. z o.o. Od 2020 r. zajmuje się \n",
    "przetwarzaniem języka naturalnego (NLP). Jego e-mail to: jan.kowalski@ai-tech.pl. \n",
    "\"To fascynująca dziedzina!\" - powiedział w wywiadzie.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== TEKST ORYGINALNY ===\")\n",
    "print(tekst)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tokenizacja zdań - NLTK\n",
    "print(\"\\n=== TOKENIZACJA ZDAŃ (NLTK) ===\")\n",
    "zdania_nltk = sent_tokenize(tekst, language='polish')\n",
    "\n",
    "for i, zdanie in enumerate(zdania_nltk, 1):\n",
    "    print(f\"{i}. {zdanie.strip()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tokenizacja słów - NLTK\n",
    "print(\"\\n=== TOKENIZACJA SŁÓW (NLTK) ===\")\n",
    "slowa_nltk = word_tokenize(tekst, language='polish')\n",
    "print(f\"Liczba tokenów: {len(slowa_nltk)}\")\n",
    "print(f\"Pierwsze 20 tokenów: {slowa_nltk[:20]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tokenizacja - spaCy (bardziej zaawansowana)\n",
    "try:\n",
    "    nlp = spacy.load(\"pl_core_news_sm\")\n",
    "    print(\"\\n=== TOKENIZACJA (spaCy - model polski) ===\")\n",
    "except:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"\\n=== TOKENIZACJA (spaCy - model angielski) ===\")\n",
    "    tekst = \"Dr. Smith works at AI-Tech Inc. since 2020. He specializes in NLP!\"\n",
    "\n",
    "doc = nlp(tekst)\n",
    "\n",
    "print(\"\\nZdania:\")\n",
    "for i, sent in enumerate(doc.sents, 1):\n",
    "    print(f\"{i}. {sent.text.strip()}\")\n",
    "\n",
    "print(f\"\\nTokeny: {[token.text for token in doc][:20]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Porównanie różnych metod tokenizacji\n",
    "tekst_prosty = \"Nie lubię tego. To jest złe!!!\"\n",
    "\n",
    "print(\"=== PORÓWNANIE METOD TOKENIZACJI ===\")\n",
    "print(f\"Tekst: {tekst_prosty}\\n\")\n",
    "\n",
    "# Metoda 1: Proste split()\n",
    "print(\"1. split():\", tekst_prosty.split())\n",
    "\n",
    "# Metoda 2: Regex\n",
    "print(\"2. Regex:\", re.findall(r'\\w+|[^\\w\\s]', tekst_prosty))\n",
    "\n",
    "# Metoda 3: NLTK\n",
    "print(\"3. NLTK:\", word_tokenize(tekst_prosty, language='polish'))\n",
    "\n",
    "# Metoda 4: spaCy\n",
    "print(\"4. spaCy:\", [token.text for token in nlp(tekst_prosty)])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Lematyzacja i Stemming\n",
    "\n",
    "### Stemming\n",
    "- Odcina końcówki słów algorytmicznie\n",
    "- Szybkie, ale mniej dokładne\n",
    "- Może tworzyć \"nie-słowa\"\n",
    "- Przykład: \"biegający\" → \"bieg\"\n",
    "\n",
    "### Lematyzacja\n",
    "- Sprowadza słowa do formy podstawowej (lematu)\n",
    "- Używa słownika i kontekstu\n",
    "- Wolniejsze, ale dokładniejsze\n",
    "- Przykład: \"biegający\" → \"biegać\"\n",
    "\n",
    "### Kiedy używać?\n",
    "- **Stemming**: wyszukiwanie, clustering\n",
    "- **Lematyzacja**: analiza semantyczna, klasyfikacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stemming - Porter Stemmer (dla angielskiego)\n",
    "porter = PorterStemmer()\n",
    "\n",
    "slowa_en = [\"running\", \"runs\", \"ran\", \"runner\", \"easily\", \"fairly\"]\n",
    "\n",
    "print(\"=== STEMMING (Porter - angielski) ===\")\n",
    "for slowo in slowa_en:\n",
    "    stem = porter.stem(slowo)\n",
    "    print(f\"{slowo:15} -> {stem}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stemming - Snowball Stemmer (wielojęzyczny)\n",
    "stemmer_pl = SnowballStemmer('polish')\n",
    "\n",
    "slowa_pl = [\"biegający\", \"biegała\", \"biegać\", \"biegnie\", \"programowanie\", \"programować\", \"programista\"]\n",
    "\n",
    "print(\"\\n=== STEMMING (Snowball - polski) ===\")\n",
    "for slowo in slowa_pl:\n",
    "    stem = stemmer_pl.stem(slowo)\n",
    "    print(f\"{slowo:20} -> {stem}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Lematyzacja - NLTK WordNet (angielski)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"\\n=== LEMATYZACJA (WordNet - angielski) ===\")\n",
    "for slowo in slowa_en:\n",
    "    lemma = lemmatizer.lemmatize(slowo, pos='v')  # pos='v' oznacza czasownik\n",
    "    print(f\"{slowo:15} -> {lemma}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Lematyzacja - spaCy (lepsze dla wielu języków)\n",
    "try:\n",
    "    nlp_pl = spacy.load(\"pl_core_news_sm\")\n",
    "    tekst_pl = \"Programiści programują programy w różnych językach programowania.\"\n",
    "    doc = nlp_pl(tekst_pl)\n",
    "    jezyk = \"polski\"\n",
    "except:\n",
    "    nlp_pl = spacy.load(\"en_core_web_sm\")\n",
    "    tekst_pl = \"The runners are running in different running competitions.\"\n",
    "    doc = nlp_pl(tekst_pl)\n",
    "    jezyk = \"angielski\"\n",
    "\n",
    "print(f\"\\n=== LEMATYZACJA (spaCy - {jezyk}) ===\")\n",
    "print(f\"Tekst: {tekst_pl}\\n\")\n",
    "\n",
    "for token in doc:\n",
    "    if not token.is_punct and not token.is_space:\n",
    "        print(f\"{token.text:20} -> {token.lemma_}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Porównanie: Stemming vs Lematyzacja\n",
    "tekst_test = \"The striped bats are hanging on their feet for best\"\n",
    "tokens = word_tokenize(tekst_test)\n",
    "\n",
    "print(\"\\n=== PORÓWNANIE: STEMMING vs LEMATYZACJA ===\")\n",
    "print(f\"{'Oryginał':<20} {'Stemming':<20} {'Lematyzacja':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for token in tokens:\n",
    "    if token.isalpha():  # tylko litery\n",
    "        stem = porter.stem(token)\n",
    "        lemma = lemmatizer.lemmatize(token)\n",
    "        print(f\"{token:<20} {stem:<20} {lemma:<20}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Usuwanie Stop Words i Normalizacja\n",
    "\n",
    "### Stop Words\n",
    "Najczęstsze słowa w języku, które zazwyczaj niosą mało znaczenia:\n",
    "- Polski: \"i\", \"w\", \"na\", \"z\", \"się\", \"jest\", \"to\"\n",
    "- Angielski: \"the\", \"is\", \"at\", \"which\", \"on\", \"a\", \"an\"\n",
    "\n",
    "### Kiedy usuwać stop words?\n",
    "- ✅ Klasyfikacja tekstu\n",
    "- ✅ Clustering dokumentów\n",
    "- ✅ Analiza tematów\n",
    "- ❌ Tłumaczenia maszynowe\n",
    "- ❌ Analiza sentymentu (\"not good\" vs \"good\")\n",
    "- ❌ Rozpoznawanie nazwanych encji\n",
    "\n",
    "### Normalizacja\n",
    "- Lowercase (małe litery)\n",
    "- Usuwanie znaków specjalnych\n",
    "- Usuwanie liczb\n",
    "- Usuwanie białych znaków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Stop words - NLTK\n",
    "print(\"=== STOP WORDS ===\")\n",
    "\n",
    "stop_words_pl = set(stopwords.words('polish'))\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "print(f\"Język polski - liczba stop words: {len(stop_words_pl)}\")\n",
    "print(f\"Przykłady PL: {list(stop_words_pl)[:20]}\")\n",
    "\n",
    "print(f\"\\nJęzyk angielski - liczba stop words: {len(stop_words_en)}\")\n",
    "print(f\"Przykłady EN: {list(stop_words_en)[:20]}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Usuwanie stop words - przykład\n",
    "tekst = \"To jest przykładowy tekst w języku polskim. Pokazuje jak usuwać stop words.\"\n",
    "\n",
    "print(\"\\n=== USUWANIE STOP WORDS ===\")\n",
    "print(f\"Oryginalny tekst:\\n{tekst}\\n\")\n",
    "\n",
    "# Tokenizacja\n",
    "tokens = word_tokenize(tekst.lower(), language='polish')\n",
    "print(f\"Tokeny: {tokens}\\n\")\n",
    "\n",
    "# Usunięcie stop words\n",
    "filtered_tokens = [token for token in tokens if token not in stop_words_pl and token.isalpha()]\n",
    "print(f\"Po usunięciu stop words: {filtered_tokens}\")\n",
    "\n",
    "# Złączenie z powrotem\n",
    "tekst_bez_stopwords = ' '.join(filtered_tokens)\n",
    "print(f\"\\nTekst bez stop words:\\n{tekst_bez_stopwords}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Pełna normalizacja tekstu - pipeline\n",
    "def normalize_text(text, remove_stopwords=True, lemmatize=True, language='polish'):\n",
    "    \"\"\"\n",
    "    Kompleksowa normalizacja tekstu.\n",
    "    \n",
    "    Args:\n",
    "        text: Tekst do normalizacji\n",
    "        remove_stopwords: Czy usunąć stop words\n",
    "        lemmatize: Czy lematyzować\n",
    "        language: Język tekstu\n",
    "    \n",
    "    Returns:\n",
    "        Znormalizowany tekst\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Usuń znaki specjalne i liczby\n",
    "    text = re.sub(r'[^a-ząćęłńóśźż\\s]', '', text)\n",
    "    \n",
    "    # 3. Tokenizacja\n",
    "    tokens = word_tokenize(text, language=language)\n",
    "    \n",
    "    # 4. Usuń stop words\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words(language))\n",
    "        tokens = [t for t in tokens if t not in stop_words]\n",
    "    \n",
    "    # 5. Lematyzacja (używamy spaCy)\n",
    "    if lemmatize:\n",
    "        try:\n",
    "            nlp = spacy.load(\"pl_core_news_sm\" if language == 'polish' else \"en_core_web_sm\")\n",
    "            doc = nlp(' '.join(tokens))\n",
    "            tokens = [token.lemma_ for token in doc if not token.is_space]\n",
    "        except:\n",
    "            pass  # Jeśli nie ma modelu, pomiń lematyzację\n",
    "    \n",
    "    # 6. Usuń puste tokeny\n",
    "    tokens = [t for t in tokens if t.strip()]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Test\n",
    "tekst_test = \"Dr. Jan Kowalski (ur. 1980) pracuje w AI-Tech!!! Zajmuje się NLP-em od 2020 roku.\"\n",
    "\n",
    "print(\"\\n=== PEŁNA NORMALIZACJA ===\")\n",
    "print(f\"Oryginał:\\n{tekst_test}\\n\")\n",
    "print(f\"Znormalizowany:\\n{normalize_text(tekst_test)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 POS Tagging (Part-of-Speech Tagging)\n",
    "\n",
    "**POS Tagging** to oznaczanie części mowy dla każdego słowa:\n",
    "- Rzeczownik (noun)\n",
    "- Czasownik (verb)\n",
    "- Przymiotnik (adjective)\n",
    "- Przysłówek (adverb)\n",
    "- itp.\n",
    "\n",
    "### Zastosowania:\n",
    "- Analiza składniowa\n",
    "- Ekstrakcja informacji\n",
    "- Poprawianie lematyzacji\n",
    "- Rozumienie struktury zdania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# POS Tagging - NLTK (angielski)\n",
    "from nltk import pos_tag\n",
    "\n",
    "tekst_en = \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens_en = word_tokenize(tekst_en)\n",
    "pos_tags = pos_tag(tokens_en)\n",
    "\n",
    "print(\"=== POS TAGGING (NLTK - angielski) ===\")\n",
    "print(f\"Tekst: {tekst_en}\\n\")\n",
    "\n",
    "for word, tag in pos_tags:\n",
    "    print(f\"{word:15} -> {tag:10}\")\n",
    "\n",
    "print(\"\\nLegenda:\")\n",
    "print(\"DT = Determiner, JJ = Adjective, NN = Noun, VBZ = Verb, IN = Preposition\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# POS Tagging - spaCy (lepsze, wielojęzyczne)\n",
    "try:\n",
    "    nlp = spacy.load(\"pl_core_news_sm\")\n",
    "    tekst = \"Szybki brązowy lis przeskoczył przez leniwego psa.\"\n",
    "    jezyk = \"polski\"\n",
    "except:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    tekst = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    jezyk = \"angielski\"\n",
    "\n",
    "doc = nlp(tekst)\n",
    "\n",
    "print(f\"\\n=== POS TAGGING (spaCy - {jezyk}) ===\")\n",
    "print(f\"Tekst: {tekst}\\n\")\n",
    "print(f\"{'Token':<20} {'POS':<10} {'Tag':<10} {'Opis'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for token in doc:\n",
    "    if not token.is_space:\n",
    "        print(f\"{token.text:<20} {token.pos_:<10} {token.tag_:<10} {spacy.explain(token.pos_)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ekstrakcja rzeczowników i czasowników\n",
    "print(\"\\n=== EKSTRAKCJA CZĘŚCI MOWY ===\")\n",
    "\n",
    "rzeczowniki = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "czasowniki = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "przymiotniki = [token.text for token in doc if token.pos_ == \"ADJ\"]\n",
    "\n",
    "print(f\"Rzeczowniki: {rzeczowniki}\")\n",
    "print(f\"Czasowniki: {czasowniki}\")\n",
    "print(f\"Przymiotniki: {przymiotniki}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Dependency Parsing\n",
    "\n",
    "**Dependency Parsing** to analiza zależności składniowych między słowami.\n",
    "\n",
    "### Co pokazuje?\n",
    "- Relacje między słowami\n",
    "- Struktura gramatyczna zdania\n",
    "- Podmiot, orzeczenie, dopełnienie\n",
    "- Modyfikatory\n",
    "\n",
    "### Zastosowania:\n",
    "- Ekstrakcja relacji\n",
    "- Rozumienie pytań\n",
    "- Analiza sentymentu zorientowana na aspekty\n",
    "- Generowanie odpowiedzi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dependency Parsing - spaCy\n",
    "try:\n",
    "    nlp = spacy.load(\"pl_core_news_sm\")\n",
    "    tekst = \"Jan dał Marii książkę o sztucznej inteligencji.\"\n",
    "except:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    tekst = \"John gave Mary a book about artificial intelligence.\"\n",
    "\n",
    "doc = nlp(tekst)\n",
    "\n",
    "print(\"=== DEPENDENCY PARSING ===\")\n",
    "print(f\"Tekst: {tekst}\\n\")\n",
    "print(f\"{'Token':<20} {'Dep':<15} {'Head':<20} {'Opis'}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text:<20} {token.dep_:<15} {token.head.text:<20} {spacy.explain(token.dep_)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Wizualizacja drzewa zależności (w notebooku)\n",
    "from spacy import displacy\n",
    "\n",
    "print(\"\\n=== WIZUALIZACJA DRZEWA ZALEŻNOŚCI ===\")\n",
    "\n",
    "# Renderowanie w notebooku\n",
    "displacy.render(doc, style=\"dep\", jupyter=True, options={'distance': 120})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Ekstrakcja podmiotu i dopełnienia\n",
    "print(\"\\n=== EKSTRAKCJA PODMIOTÓW I DOPEŁNIEŃ ===\")\n",
    "\n",
    "for token in doc:\n",
    "    if \"subj\" in token.dep_:  # podmiot\n",
    "        print(f\"Podmiot: {token.text}\")\n",
    "    if \"obj\" in token.dep_:  # dopełnienie\n",
    "        print(f\"Dopełnienie: {token.text}\")\n",
    "    if token.pos_ == \"VERB\":\n",
    "        print(f\"Czasownik: {token.text}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ćwiczenie kompleksowe\n",
    "\n",
    "**Zadanie**: Stwórz pipeline do przetwarzania tekstu, który:\n",
    "1. Normalizuje tekst\n",
    "2. Usuwa stop words\n",
    "3. Lematyzuje słowa\n",
    "4. Wyciąga rzeczowniki i czasowniki\n",
    "5. Znajduje nazwane encje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def advanced_text_processing(text, language='polish'):\n",
    "    \"\"\"\n",
    "    Kompleksowe przetwarzanie tekstu.\n",
    "    \"\"\"\n",
    "    # Załaduj model spaCy\n",
    "    try:\n",
    "        if language == 'polish':\n",
    "            nlp = spacy.load(\"pl_core_news_sm\")\n",
    "        else:\n",
    "            nlp = spacy.load(\"en_core_web_sm\")\n",
    "    except:\n",
    "        print(\"⚠️ Model nie jest zainstalowany\")\n",
    "        return None\n",
    "    \n",
    "    # Przetwórz tekst\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # 1. Podstawowe informacje\n",
    "    print(\"=== ANALIZA TEKSTU ===\")\n",
    "    print(f\"Tekst: {text}\\n\")\n",
    "    print(f\"Liczba tokenów: {len(doc)}\")\n",
    "    print(f\"Liczba zdań: {len(list(doc.sents))}\\n\")\n",
    "    \n",
    "    # 2. Lematyzacja\n",
    "    print(\"=== LEMATY ===\")\n",
    "    lemmas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "    print(lemmas)\n",
    "    print()\n",
    "    \n",
    "    # 3. Części mowy\n",
    "    print(\"=== CZĘŚCI MOWY ===\")\n",
    "    rzeczowniki = [token.text for token in doc if token.pos_ == \"NOUN\"]\n",
    "    czasowniki = [token.text for token in doc if token.pos_ == \"VERB\"]\n",
    "    print(f\"Rzeczowniki: {rzeczowniki}\")\n",
    "    print(f\"Czasowniki: {czasowniki}\\n\")\n",
    "    \n",
    "    # 4. Nazwane encje\n",
    "    print(\"=== NAZWANE ENCJE ===\")\n",
    "    if doc.ents:\n",
    "        for ent in doc.ents:\n",
    "            print(f\"{ent.text:<30} -> {ent.label_:<15}\")\n",
    "    else:\n",
    "        print(\"Nie znaleziono nazwanych encji.\")\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Test\n",
    "tekst_test = \"\"\"\n",
    "Adam Mickiewicz napisał Pana Tadeusza w Paryżu. \n",
    "To arcydzieło polskiej literatury romantycznej ukazało się w 1834 roku.\n",
    "\"\"\"\n",
    "\n",
    "doc = advanced_text_processing(tekst_test, language='polish')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Podsumowanie Modułu 3\n",
    "\n",
    "✅ Nauczyliśmy się tokenizacji na różne sposoby\n",
    "\n",
    "✅ Zrozumieliśmy różnicę między stemmingiem a lematyzacją\n",
    "\n",
    "✅ Poznaliśmy techniki normalizacji i usuwania stop words\n",
    "\n",
    "✅ Opanowaliśmy POS tagging i dependency parsing\n",
    "\n",
    "✅ Stworzyliśmy kompleksowy pipeline przetwarzania tekstu\n",
    "\n",
    "---\n",
    "\n",
    "**Następny krok**: Warsztaty praktyczne - Zastosowanie poznanych technik w praktyce!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
